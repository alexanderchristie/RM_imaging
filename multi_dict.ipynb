{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1450, 400)\n",
      "(400, 400)\n",
      "coherence of data:  0.7119395644427655\n"
     ]
    }
   ],
   "source": [
    "#Notebooks for small tests\n",
    "import os  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "plt.rcParams['axes.facecolor']='w'\n",
    "plt.rcParams['savefig.facecolor']='w'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "#imports and plotting function\n",
    "import argparse\n",
    "import mat73\n",
    "import logging\n",
    "import numpy as np\n",
    "#import torchvision\n",
    "from datetime import datetime\n",
    "import os\n",
    "import scipy.io\n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import Models as M\n",
    "import Helpers as H\n",
    "import time\n",
    "from torch.func import vmap\n",
    "from functorch.experimental import replace_all_batch_norm_modules_\n",
    "%matplotlib inline\n",
    "\n",
    "encoder_out='sigmoid'\n",
    "cwd=os. getcwd()\n",
    "\n",
    "data_path=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "print(medium.shape)\n",
    "inners=medium.transpose().conjugate()@medium\n",
    "print(inners.shape)\n",
    "coherence=0\n",
    "for i in inners:\n",
    "    for j in i:\n",
    "        if abs(j)>coherence and j<.99:\n",
    "            coherence=abs(j)\n",
    "print('coherence of data: ', coherence)\n",
    "#b=np.load(data_path+'/train/b.npy')\n",
    "#rho=np.load(data_path+'/train/rho.npy')\n",
    "#print(np.allclose(medium@rho[0],b[0]))\n",
    "index_list=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data training\n",
    "starttime=time.time()\n",
    "target='raw'\n",
    "labeled_data=2000\n",
    "layer_loss_list=[]\n",
    "Epochs=10\n",
    "Track_run=False\n",
    "#Enter your wanbd key below and uncomment wanbd code to track run on wandb\n",
    "if Track_run:\n",
    "    key='ENTER YOUR KEY HERE' #enter your own key here\n",
    "    wandb.login(key=key) \n",
    "    wandb.init(project='Test runs')\n",
    "    wandb.config['labeled_data']=labeled_data\n",
    "wand_dict={}\n",
    "batchsize=128\n",
    "layers=[5000,5000, 5000, 3000, 3000, 4000,8000]\n",
    "training_data=H.data_rho_loaded(data_path+'/train',labeled_data/80000)\n",
    "trainloader=DataLoader(training_data,batch_size=batchsize,shuffle=True,num_workers=4)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax=nn.Softmax(dim=0)\n",
    "val_data=H.data_rho_loaded(data_path+'/val', 3000/80000)\n",
    "valloader=DataLoader(val_data,batch_size=512,shuffle=True,num_workers=4)\n",
    "encoder=M.fc_net_extra(training_data.b[0].shape[0]/2, layers, training_data.rho[0].shape[0]/4, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None)\n",
    "print(H.count_parameters(encoder))\n",
    "print('Data shapes:', training_data.b[0].shape[0]/2, training_data.rho[0].shape[0]/4)\n",
    "encoder.to(device)\n",
    "\n",
    "\n",
    "relu=nn.ReLU()\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "bce_loss_fn=nn.BCELoss()   \n",
    "L2_loss_fn=nn.MSELoss()\n",
    "for epoch in range(Epochs):\n",
    "    train_lossavg=0\n",
    "    train_lossavg=0\n",
    "    encoder.train()\n",
    "    for batch, (b,rho,num_targets) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        b=b.to(device)\n",
    "        rho=rho.to(device)    \n",
    "        if labeled_data==1:\n",
    "            b=b.squeeze().unsqueeze(0).repeat(128,1).unsqueeze(1)  \n",
    "            rho=rho.squeeze().unsqueeze(0).repeat(128,1).unsqueeze(1)\n",
    "        font_size = 50\n",
    "        rho_hat=encoder(b)\n",
    "        rh, _=torch.split(rho, 400, dim=-1)\n",
    "        if encoder_out=='sigmoid':\n",
    "            #rho_hat=rho_hat-rho\n",
    "            rho_hat=sigmoid(rho_hat)\n",
    "        elif encoder_out=='softmax':\n",
    "            rho_hat=5*softmax(rho_hat.squeeze())\n",
    "        elif encoder_out=='relu':\n",
    "            rho_hat=relu(rho_hat.squeeze())\n",
    "        #rho_hat=relu(rho_hat.squeeze())\n",
    "        #H.plot_2_imgs(rho, rho_hat,ind=5, figsize=5)    \n",
    "        if encoder_out=='sigmoid' or encoder_out=='softmax':\n",
    "            #print(rho_hat.shape, rh.shape)\n",
    "            bce_loss=bce_loss_fn(rho_hat.squeeze(), rh.squeeze())\n",
    "            bce_loss.backward() \n",
    "            train_lossavg+=bce_loss.item()/len(trainloader)\n",
    "\n",
    "        else:\n",
    "            L2_loss=L2_loss_fn(rho_hat.squeeze(), rh.squeeze())    \n",
    "            L2_loss.backward()\n",
    "            train_lossavg+=L2_loss.item()/len(trainloader)\n",
    "        optimizer.step()\n",
    "    if epoch==Epochs-1:\n",
    "        H.plot_2_imgs(rho, rho_hat,ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "        finish=time.time()-starttime\n",
    "        print(f\"time for labeled: {finish}\")\n",
    "    wand_dict['train loss']=train_lossavg\n",
    "    val_loss=0\n",
    "    encoder.eval()\n",
    "    for batch, (b,rho,num_targets) in enumerate(valloader):\n",
    "        b=b.to(device)\n",
    "        rho=rho.to(device)    \n",
    "        font_size = 50\n",
    "        rho_hat=encoder(b)\n",
    "        if encoder_out=='sigmoid':\n",
    "            rho_hat=sigmoid(rho_hat)\n",
    "        elif encoder_out=='softmax':\n",
    "            rho_hat=5*softmax(rho_hat.squeeze())\n",
    "        elif encoder_out=='relu':\n",
    "            rho_hat=relu(rho_hat.squeeze())\n",
    "        #rho_hat=relu(rho_hat.squeeze())\n",
    "        #H.plot_2_imgs(rho, rho_hat,ind=5, figsize=5)    \n",
    "        rh, _=torch.split(rho, 400, dim=-1)\n",
    "        if encoder_out=='sigmoid' or encoder_out=='softmax':\n",
    "\n",
    "            bce_loss=bce_loss_fn(rho_hat.squeeze(), rh.squeeze())\n",
    "            val_loss+=bce_loss.item()/len(valloader)\n",
    "        else:\n",
    "            L2_loss=L2_loss_fn(rho_hat.squeeze(), rh.squeeze())    \n",
    "            val_loss+=L2_loss.item()/len(valloader)\n",
    "    wand_dict['val loss']=val_loss\n",
    "    if epoch==Epochs-1:\n",
    "       H.plot_2_imgs(rho, rho_hat,ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "    if Track_run:\n",
    "        wandb.log(wand_dict)\n",
    "\n",
    "if Track_run:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium: (1450, 400), Rho: (40000, 400), B: (40000, 1450)\n",
      "Medium: (1450, 400), Rho: (3000, 400), B: (3000, 1450)\n",
      "8281000\n",
      "Data shapes: 1450.0 200.0\n",
      "torch.Size([128, 2900]) torch.Size([400, 256]) torch.Size([400, 128]) torch.Size([400, 2900])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 128 but got size 400 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m km_b\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat((km_b_real, km_b_imag), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mprint\u001b[39m(b\u001b[38;5;241m.\u001b[39mshape, km_b\u001b[38;5;241m.\u001b[39mshape,km_b_real\u001b[38;5;241m.\u001b[39mshape, medium_hat\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 115\u001b[0m In_put\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkm_b\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m b\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    120\u001b[0m rho\u001b[38;5;241m=\u001b[39mrho\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 128 but got size 400 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "#Raw data training unlabeled only\n",
    "starttime=time.time()\n",
    "target='raw'\n",
    "unlabeled_data=40000\n",
    "batchsize=128\n",
    "L1_weight=1e-10\n",
    "layers=[500,500,500,300,300,400,800]\n",
    "#layers.append(256)\n",
    "GELMA_layers=[500,500,500,300,300,400,800]\n",
    "CE=False\n",
    "KM_in=False\n",
    "#GELMA_layers.append(256)\n",
    "\n",
    "GELMA=0\n",
    "G_0=True\n",
    "Epochs=1000\n",
    "index_list=[]\n",
    "\n",
    "\n",
    "Track_run=False\n",
    "#Enter your wanbd key below and uncomment wanbd code to track run on wandb\n",
    "if Track_run:\n",
    "    key='ENTER YOUR KEY HERE' #enter your own key here\n",
    "    wandb.login(key=key) \n",
    "    wandb.init(project='Test runs')\n",
    "    wandb.config['labeled_data']=labeled_data\n",
    "wand_dict={}\n",
    "#GELMA_layers.append(500)\n",
    "#layers.append(500)\n",
    "training_data=H.data_rho_loaded(data_path+'/train',unlabeled_data/80000)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax=nn.Softmax(dim=0)\n",
    "#training_data.b=training_data.b.to(device)\n",
    "#training_data.rho=training_data.rho.to(device)\n",
    "#rh_t, _=torch.split(rho, 400, dim=-1)\n",
    "\n",
    "\n",
    "val_data=H.data_rho_loaded(data_path+'/val', 3000/80000)\n",
    "b_val=val_data.b.to(device)\n",
    "rho_val=val_data.rho.to(device)\n",
    "rh_v, _=torch.split(rho_val, 400, dim=-1)\n",
    "indim=int(training_data.b[0].shape[0]/2)\n",
    "outdim=int(training_data.rho[0].shape[0]/4)\n",
    "enc_dim=training_data.b[0].shape[0]/2\n",
    "if KM_in:\n",
    "    enc_dim=enc_dim*2\n",
    "\n",
    "\n",
    "encoder=M.fc_net_extra(enc_dim, layers,outdim , net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None,dropout=.5)\n",
    "decoder=nn.Linear(int(training_data.rho[0].shape[0]/2), int(training_data.b[0].shape[0]), bias=False)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "if G_0:\n",
    "    G_0=(np.array(mat73.loadmat(data_path+'/G_0.mat')['A0']))\n",
    "    G_0_w=torch.cat((torch.tensor(G_0.real), torch.tensor(G_0.imag)), dim=0)\n",
    "    G_0_w=G_0_w.float()\n",
    "    decoder.weight.data=nn.parameter.Parameter(G_0_w.clone().detach().requires_grad_(True))\n",
    "\n",
    "\n",
    "print(H.count_parameters(encoder))\n",
    "print('Data shapes:', training_data.b[0].shape[0]/2, training_data.rho[0].shape[0]/4)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "relu=nn.ReLU()\n",
    "optimizer = torch.optim.AdamW(encoder.parameters(), lr=0.001)\n",
    "optimizer_decod = torch.optim.AdamW(decoder.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "softmax=nn.Softmax(dim=1)\n",
    "bce_loss_fn=nn.BCELoss()   \n",
    "L2_loss_fn=nn.MSELoss()\n",
    "CE_loss_fn=nn.CrossEntropyLoss()\n",
    "L2_loss_fn=lambda x,y: torch.sqrt(nn.MSELoss()(x,y))\n",
    "L1_loss_fn=nn.L1Loss()\n",
    "Complex_eye=torch.eye(int(outdim*2)).unsqueeze(1).to(device)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "if GELMA>0:\n",
    "    GELMA_net=M.fc_net_batch(training_data.b[0].shape[0]/2, GELMA_layers, training_data.b[0].shape[0]/2, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None, dropout=.5)\n",
    "    optimizer_GELMA = torch.optim.Adam(GELMA_net.parameters(), lr=.001, maximize=True)\n",
    "    GELMA_net.to(device)\n",
    "    GELMA_net=nn.DataParallel(GELMA_net)\n",
    "    GELMA_net.train()\n",
    "def f_col(batch):\n",
    "    b=torch.stack([item[0] for item in batch])\n",
    "    rho=torch.stack([item[1] for item in batch])\n",
    "    num_targets=torch.stack([item[2] for item in batch])\n",
    "    return b.to(device), rho.to(device), num_targets\n",
    "lr_scheduler_enc=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.995, last_epoch=-1)\n",
    "lr_scheduler_decod=torch.optim.lr_scheduler.ExponentialLR(optimizer_decod, gamma=.995, last_epoch=-1)\n",
    "trainloader=DataLoader(training_data,batch_size=batchsize,shuffle=True,num_workers=0)\n",
    "for epoch in range(Epochs):\n",
    "    #if batchsize<unlabeled_data:\n",
    "    #    perm=torch.randperm(unlabeled_data)\n",
    "    #    b=b[perm]\n",
    "    #    rho=rho[perm]\n",
    "    #for chunk in range(0, unlabeled_data, batchsize): \n",
    "    #    b_chunk=b[chunk:chunk+batchsize]\n",
    "    #    rho_chunk=rho[chunk:chunk+batchsize]   \n",
    "    train_lossavg=0\n",
    "    train_lossavg=0\n",
    "    for batch, (b, rho, num_targets) in enumerate(trainloader):\n",
    "        if True:\n",
    "            medium_hat=decoder(Complex_eye).squeeze()\n",
    "            medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "            Mhat_real, Mhat_imag=torch.split(medium_hat, int(medium_hat.shape[1]/2), dim=-1)\n",
    "            \n",
    "            b_real, b_imag=torch.split(b.squeeze(), int(b.shape[-1]/2), dim=-1)\n",
    "            km_b_real=Mhat_real.squeeze()@b_real.squeeze().T-Mhat_imag.squeeze()@b_imag.squeeze().T\n",
    "            km_b_imag=Mhat_real.squeeze()@b_imag.squeeze().T+Mhat_imag.squeeze()@b_real.squeeze().T\n",
    "\n",
    "            km_b=torch.cat((km_b_real, km_b_imag), dim=-1)\n",
    "            \n",
    "            print(b.shape, km_b.shape,km_b_real.shape, medium_hat.shape)\n",
    "            In_put=torch.cat((b, km_b), dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "            b=b.to(device)\n",
    "            rho=rho.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_decod.zero_grad()\n",
    "            if KM_in:\n",
    "                rho_hat=encoder(In_put)\n",
    "            else:\n",
    "                rho_hat=encoder(b)\n",
    "            rho_hat=sigmoid(rho_hat)\n",
    "            b_hat=decoder(rho_hat)\n",
    "            rho_hat_hat=encoder(b_hat)\n",
    "\n",
    "            if L1_weight>0:\n",
    "                L1_loss=L1_weight*L1_loss_fn(rho_hat, rho_hat*0)\n",
    "                L1_loss.backward(retain_graph=True)  \n",
    "                L1_loss=L1_loss.item()/L1_weight\n",
    "            else:\n",
    "                L1_loss=-1\n",
    "\n",
    "            if CE:\n",
    "                CEloss=CE_loss_fn(rho_hat_hat, rho_hat)/1000\n",
    "                CEloss.backward(retain_graph=True)\n",
    "                CEloss=CEloss.item()\n",
    "\n",
    "            \n",
    "            if GELMA>0 and epoch>100:\n",
    "                optimizer_GELMA.zero_grad()\n",
    "                GELMA_out=GELMA_net(b)\n",
    "                GELMA_inners=torch.inner(GELMA_out.squeeze(), (b-b_hat).squeeze()).diagonal(dim1=-2, dim2=-1)\n",
    "                inner_loss_term=GELMA*sum((GELMA_inners))/len(GELMA_inners)\n",
    "                inner_loss_term.backward(retain_graph=True)\n",
    "                inner_loss_term_avg=inner_loss_term.item()/GELMA\n",
    "                optimizer_GELMA.step()\n",
    "                \n",
    "            else:\n",
    "                inner_loss_term_avg=-1\n",
    "            L2_loss=L2_loss_fn(b_hat.squeeze(), b.squeeze())\n",
    "            L2_loss.backward()\n",
    "\n",
    "\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer_decod.step()\n",
    "            train_lossavg+=L2_loss.item()\n",
    "    #if L1_loss>5e-5:\n",
    "    #    L1_weight=L1_weight*(0.9)\n",
    "    val_lossavg=0\n",
    "    if epoch%100==0:\n",
    "       H.plot_2_imgs(rho, rho_hat,ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_decod.zero_grad()\n",
    "\n",
    "    font_size = 50\n",
    "    rho_hat=encoder(b_val)\n",
    "    rho_hat=sigmoid(rho_hat)\n",
    "    b_hat=decoder(rho_hat)\n",
    "    L2_loss=L2_loss_fn(b_hat.squeeze(), b_val.squeeze())\n",
    "    val_lossavg+=L2_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "   \n",
    "    if epoch%10000==0:\n",
    "       H.plot_2_imgs(rho_val, rho_hat,ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "       \n",
    "    if epoch%1==0:\n",
    "        sum_max_inner_original=0\n",
    "        medium_hat=decoder(Complex_eye).squeeze()\n",
    "        medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "        medium_hat=medium_hat.cpu().detach().numpy()\n",
    "        medium_hat=H.cat2complex(medium_hat)                    \n",
    "        torch_inners=np.abs(np.inner(medium_hat,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "                #original way of computing inners\n",
    "                # mean over true max over hats\n",
    "        for i in range(int(outdim*2)):\n",
    "            if max(torch_inners[:, i])>.95 and i not in index_list:\n",
    "                index_list.append(i)\n",
    "            sum_max_inner_original=sum_max_inner_original+max(torch_inners[:, i])\n",
    "        max_avg_inners_original=sum_max_inner_original/(int(outdim*2))\n",
    " \n",
    "        \n",
    "        #lr_scheduler_enc.step()\n",
    "        #lr_scheduler_decod.step()\n",
    "        \n",
    "        \n",
    "        print(f'epoch: {epoch}, train loss: {train_lossavg}, L1 loss: {L1_loss}, val loss: {val_lossavg}, max avg inners: {max_avg_inners_original}, GELMA loss: {inner_loss_term_avg}, num indices: {len(index_list)}')\n",
    "        \n",
    "    if Track_run:\n",
    "        wandb.log(wand_dict)\n",
    "\n",
    "#index_list_list.append(index_list)\n",
    "print(time.time()-starttime)\n",
    "if Track_run:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium: (1450, 400), Rho: (5000, 400), B: (5000, 1450)\n",
      "Medium: (1450, 400), Rho: (3000, 400), B: (3000, 1450)\n",
      "11046400\n",
      "Data shapes: 1450.0 200.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, train loss: 0.5128243193030357, KL_LOSS: 185920.2484375, val loss: 0.039056263864040375, max avg inners: 0.02431056092551005,  num indices: 0\n",
      "epoch: 1, train loss: 0.0896213211119175, KL_LOSS: 126730.4671875, val loss: 0.03776675462722778, max avg inners: 0.03329611467655931,  num indices: 0\n",
      "epoch: 2, train loss: 0.05555113591253757, KL_LOSS: 87099.29257812498, val loss: 0.037269044667482376, max avg inners: 0.04939327193757353,  num indices: 0\n",
      "epoch: 3, train loss: 0.04503376781940461, KL_LOSS: 58947.5921875, val loss: 0.03705944865942001, max avg inners: 0.06510444640738919,  num indices: 0\n",
      "epoch: 4, train loss: 0.0408379141241312, KL_LOSS: 38984.0875, val loss: 0.03696969524025917, max avg inners: 0.07305109542838405,  num indices: 0\n",
      "epoch: 5, train loss: 0.03950601965188981, KL_LOSS: 25000.966406249998, val loss: 0.036939457058906555, max avg inners: 0.0754046671508731,  num indices: 0\n",
      "epoch: 6, train loss: 0.03910431340336799, KL_LOSS: 17031.8630859375, val loss: 0.03697577118873596, max avg inners: 0.07378322695919189,  num indices: 0\n",
      "epoch: 7, train loss: 0.03865961506962776, KL_LOSS: 12622.855761718749, val loss: 0.037137534469366074, max avg inners: 0.06752719935596432,  num indices: 0\n",
      "epoch: 8, train loss: 0.038741808384656906, KL_LOSS: 9781.93251953125, val loss: 0.037436772137880325, max avg inners: 0.06178089667627776,  num indices: 0\n",
      "epoch: 9, train loss: 0.038740746304392816, KL_LOSS: 7654.32470703125, val loss: 0.03786683455109596, max avg inners: 0.05654732027210365,  num indices: 0\n",
      "epoch: 10, train loss: 0.0387708816677332, KL_LOSS: 6073.353173828124, val loss: 0.039516083896160126, max avg inners: 0.04922884760004825,  num indices: 0\n",
      "epoch: 11, train loss: 0.038637121021747586, KL_LOSS: 4849.394091796875, val loss: 0.040295008569955826, max avg inners: 0.04926470935292406,  num indices: 0\n",
      "epoch: 12, train loss: 0.038617762923240664, KL_LOSS: 3906.969409179688, val loss: 0.04288577288389206, max avg inners: 0.05120894508492114,  num indices: 0\n",
      "epoch: 13, train loss: 0.03871193043887615, KL_LOSS: 3084.894775390625, val loss: 0.04336755350232124, max avg inners: 0.04937970089542652,  num indices: 0\n",
      "epoch: 14, train loss: 0.03854113295674324, KL_LOSS: 2489.4465820312503, val loss: 0.04089367389678955, max avg inners: 0.05038099727378012,  num indices: 0\n",
      "epoch: 15, train loss: 0.03832209296524525, KL_LOSS: 1980.820446777344, val loss: 0.04072634503245354, max avg inners: 0.05304700080264774,  num indices: 0\n",
      "epoch: 16, train loss: 0.03820866458117962, KL_LOSS: 1624.176428222656, val loss: 0.041747696697711945, max avg inners: 0.05557311968534713,  num indices: 0\n",
      "epoch: 17, train loss: 0.03832693360745907, KL_LOSS: 1352.2213134765625, val loss: 0.04405910521745682, max avg inners: 0.05654373903476001,  num indices: 0\n",
      "epoch: 18, train loss: 0.03840183168649673, KL_LOSS: 1096.1861328125, val loss: 0.04148396849632263, max avg inners: 0.05341515130804062,  num indices: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m L1_weight\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     99\u001b[0m     L1_loss\u001b[38;5;241m=\u001b[39mL1_weight\u001b[38;5;241m*\u001b[39mL1_loss_fn(rho_hat, rho_hat\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 100\u001b[0m     \u001b[43mL1_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \n\u001b[1;32m    101\u001b[0m     L1_loss\u001b[38;5;241m=\u001b[39mL1_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39mL1_weight\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Workstation1/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Workstation1/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Raw data training variational autoencoder (KL replcaed with BCE)\n",
    "starttime=time.time()\n",
    "target='raw'\n",
    "unlabeled_data=5000\n",
    "batchsize=128\n",
    "layers=[500,500,500,300,300,300,400,800]\n",
    "G_0=False\n",
    "Epochs=10000\n",
    "index_list=[]\n",
    "KL_weight=1\n",
    "L1_weight=1e-3\n",
    "\n",
    "Track_run=False\n",
    "#Enter your wanbd key below and uncomment wanbd code to track run on wandb\n",
    "if Track_run:\n",
    "    key='ENTER YOUR KEY HERE' #enter your own key here\n",
    "    wandb.login(key=key) \n",
    "    wandb.init(project='Test runs')\n",
    "    wandb.config['labeled_data']=labeled_data\n",
    "wand_dict={}\n",
    "batchsize=512\n",
    "#GELMA_layers.append(500)\n",
    "#layers.append(500)\n",
    "training_data=H.data_rho_loaded(data_path+'/train',unlabeled_data/80000)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax=nn.Softmax(dim=0)\n",
    "#training_data.b=training_data.b.to(device)\n",
    "#training_data.rho=training_data.rho.to(device)\n",
    "#rh_t, _=torch.split(rho, 400, dim=-1)\n",
    "\n",
    "\n",
    "val_data=H.data_rho_loaded(data_path+'/val', 3000/80000)\n",
    "b_val=val_data.b.to(device)\n",
    "rho_val=val_data.rho.to(device)\n",
    "rh_v, _=torch.split(rho_val, 400, dim=-1)\n",
    "indim=int(training_data.b[0].shape[0]/2)\n",
    "outdim=int(training_data.rho[0].shape[0]/4)\n",
    "\n",
    "encoder=M.variational_enc(training_data.b[0].shape[0]/2, layers,outdim , net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None,dropout=.5)\n",
    "decoder=nn.Linear(int(training_data.rho[0].shape[0]/2), int(training_data.b[0].shape[0]), bias=False)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "decoder=M.fc_net_extra(outdim, [layers[len(layers)-i-1] for i in range(len(layers))],training_data.b[0].shape[0]/2 , net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None,dropout=.5)\n",
    "\n",
    "if G_0:\n",
    "    G_0=(np.array(mat73.loadmat(data_path+'/G_0.mat')['A0']))\n",
    "    G_0_w=torch.cat((torch.tensor(G_0.real), torch.tensor(G_0.imag)), dim=0)\n",
    "    G_0_w=G_0_w.float()\n",
    "    decoder.weight.data=nn.parameter.Parameter(G_0_w.clone().detach().requires_grad_(True))\n",
    "\n",
    "\n",
    "print(H.count_parameters(encoder))\n",
    "print('Data shapes:', training_data.b[0].shape[0]/2, training_data.rho[0].shape[0]/4)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "relu=nn.ReLU()\n",
    "optimizer = torch.optim.AdamW(encoder.parameters(), lr=0.001)\n",
    "optimizer_decod = torch.optim.AdamW(decoder.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "softmax=nn.Softmax(dim=1)\n",
    "L1_loss_fn=nn.L1Loss()\n",
    "L2_loss_fn=nn.MSELoss()\n",
    "CE_loss_fn=nn.CrossEntropyLoss()\n",
    "L2_loss_fn=lambda x,y: torch.sqrt(nn.MSELoss()(x,y))\n",
    "Complex_eye=torch.eye(int(outdim*2)).unsqueeze(1).to(device)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "\n",
    "lr_scheduler_enc=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.995, last_epoch=-1)\n",
    "lr_scheduler_decod=torch.optim.lr_scheduler.ExponentialLR(optimizer_decod, gamma=.995, last_epoch=-1)\n",
    "trainloader=DataLoader(training_data,batch_size=batchsize,shuffle=True,num_workers=0)\n",
    "for epoch in range(Epochs):\n",
    "    #if batchsize<unlabeled_data:\n",
    "    #    perm=torch.randperm(unlabeled_data)\n",
    "    #    b=b[perm]\n",
    "    #    rho=rho[perm]\n",
    "    #for chunk in range(0, unlabeled_data, batchsize): \n",
    "    #    b_chunk=b[chunk:chunk+batchsize]\n",
    "    #    rho_chunk=rho[chunk:chunk+batchsize]   \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    KL_lossavg=0\n",
    "    train_lossavg=0\n",
    "    for batch, (b, rho, num_targets) in enumerate(trainloader):\n",
    "        b=b.to(device)\n",
    "        rho=rho.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_decod.zero_grad()\n",
    "\n",
    "        rho_hat_mean, rho_hat_logvar=encoder(b)\n",
    "        rho_hat=rho_hat_mean+torch.randn_like(rho_hat_logvar)* torch.exp(0.5*rho_hat_logvar)\n",
    "        KLloss=H.KL_divergence(rho_hat_mean, rho_hat_logvar)*KL_weight\n",
    "        KLloss.backward(retain_graph=True)\n",
    "        #rho_hat=sigmoid(rho_hat)\n",
    "\n",
    "        if L1_weight>0:\n",
    "            L1_loss=L1_weight*L1_loss_fn(rho_hat, rho_hat*0)\n",
    "            L1_loss.backward(retain_graph=True)  \n",
    "            L1_loss=L1_loss.item()/L1_weight\n",
    "        else:\n",
    "            L1_loss=-1\n",
    "\n",
    "        b_hat=decoder(rho_hat)\n",
    "        L2_loss=L2_loss_fn(b_hat.squeeze(), b.squeeze())\n",
    "        L2_loss.backward()\n",
    "\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer_decod.step()\n",
    "        train_lossavg+=L2_loss.item()/len(trainloader)\n",
    "        KL_lossavg+=KLloss.item()/len(trainloader)/KL_weight\n",
    "    val_lossavg=0\n",
    "    if epoch%100==0:\n",
    "       H.plot_2_imgs(rho, rho_hat,ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_decod.zero_grad()\n",
    "\n",
    "    font_size = 50\n",
    "    rho_hat_mean, rho_hat_logvar=encoder(b_val)\n",
    "    rho_hat=rho_hat_mean+torch.randn_like(rho_hat_logvar)* torch.exp(0.5*rho_hat_logvar)\n",
    "    #rho_hat=softmax(rho_hat)\n",
    "    #rho_hat=sigmoid(rho_hat)\n",
    "    b_hat=decoder(rho_hat)\n",
    "    L2_loss=L2_loss_fn(b_hat.squeeze(), b_val.squeeze())\n",
    "    val_lossavg+=L2_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "   \n",
    "    if epoch%10000==0:\n",
    "       H.plot_2_imgs(rho_val, rho_hat,ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "       \n",
    "    if epoch%1==0:\n",
    "        sum_max_inner_original=0\n",
    "        #Complex_eye=Complex_eye+\n",
    "        medium_hat=decoder(Complex_eye).squeeze()\n",
    "        medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "        medium_hat=medium_hat.cpu().detach().numpy()\n",
    "        medium_hat=H.cat2complex(medium_hat)                    \n",
    "        torch_inners=np.abs(np.inner(medium_hat,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "                #original way of computing inners\n",
    "                # mean over true max over hats\n",
    "        for i in range(int(outdim*2)):\n",
    "            if max(torch_inners[:, i])>.95 and i not in index_list:\n",
    "                index_list.append(i)\n",
    "            sum_max_inner_original=sum_max_inner_original+max(torch_inners[:, i])\n",
    "        max_avg_inners_original=sum_max_inner_original/(int(outdim*2))\n",
    " \n",
    "        \n",
    "        #lr_scheduler_enc.step()\n",
    "        #lr_scheduler_decod.step()\n",
    "        \n",
    "        \n",
    "        print(f'epoch: {epoch}, train loss: {train_lossavg}, KL_LOSS: {KL_lossavg}, val loss: {val_lossavg}, max avg inners: {max_avg_inners_original},  num indices: {len(index_list)}')\n",
    "        \n",
    "    if Track_run:\n",
    "        wandb.log(wand_dict)\n",
    "\n",
    "#index_list_list.append(index_list)\n",
    "print(time.time()-starttime)\n",
    "if Track_run:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "variational_enc(\n",
       "  (shared): fc_net_batch(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=2900, out_features=1000, bias=True)\n",
       "          (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (sigmoid): Sigmoid()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (enc_mean): fc_net_batch(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1000, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2-3): 2 x linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=600, out_features=600, bias=True)\n",
       "          (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=600, out_features=800, bias=True)\n",
       "          (1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=800, out_features=1600, bias=True)\n",
       "          (1): BatchNorm1d(1600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1600, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (sigmoid): Sigmoid()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (enc_logvar): fc_net_batch(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (1): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): linear_layer_wrapper(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=1000, out_features=400, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (sigmoid): Sigmoid()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data training unlabeled only. Encoder decoder alternation\n",
    "starttime=time.time()\n",
    "target='raw'\n",
    "unlabeled_data=10000\n",
    "L1_weight=0.005\n",
    "GELMA=0.0\n",
    "G_0=False\n",
    "Epochs=100000\n",
    "layers=[128]*14\n",
    "GELMA_layers=[128]*5\n",
    "GELMA_layers.append(500)\n",
    "layers.append(500)\n",
    "\n",
    "\n",
    "\n",
    "Track_run=False\n",
    "#Enter your wanbd key below and uncomment wanbd code to track run on wandb\n",
    "if Track_run:\n",
    "    key='ENTER YOUR KEY HERE' #enter your own key here\n",
    "    wandb.login(key=key) \n",
    "    wandb.init(project='Test runs')\n",
    "    wandb.config['labeled_data']=labeled_data\n",
    "wand_dict={}\n",
    "batchsize=128\n",
    "\n",
    "#training_data=H.data_rho_loaded(data_path+'/train',unlabeled_data/80000)\n",
    "#trainloader=DataLoader(training_data,batch_size=batchsize,shuffle=True,num_workers=4)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax=nn.Softmax(dim=0)\n",
    "b=training_data.b.to(device)\n",
    "rho=training_data.rho.to(device)\n",
    "rh_t, _=torch.split(rho, 400, dim=-1)\n",
    "\n",
    "\n",
    "#val_data=H.data_rho_loaded(data_path+'/val', 3000/80000)\n",
    "b_val=val_data.b.to(device)\n",
    "rho_val=val_data.rho.to(device)\n",
    "rh_v, _=torch.split(rho, 400, dim=-1)\n",
    "indim=int(training_data.b[0].shape[0]/2)\n",
    "outdim=int(training_data.rho[0].shape[0]/4)\n",
    "\n",
    "encoder=M.fc_net_extra(training_data.b[0].shape[0]/2, layers,outdim , net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None,dropout=.5)\n",
    "decoder=nn.Linear(int(training_data.rho[0].shape[0]/2), int(training_data.b[0].shape[0]), bias=False)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "if G_0:\n",
    "    G_0=(np.array(mat73.loadmat(data_path+'/G_0.mat')['A0']))\n",
    "    G_0_w=torch.cat((torch.tensor(G_0.real), torch.tensor(G_0.imag)), dim=0)\n",
    "    G_0_w=G_0_w.float()\n",
    "    decoder.weight.data=nn.parameter.Parameter(G_0_w.clone().detach().requires_grad_(True))\n",
    "\n",
    "\n",
    "print(H.count_parameters(encoder))\n",
    "print('Data shapes:', training_data.b[0].shape[0]/2, training_data.rho[0].shape[0]/4)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "relu=nn.ReLU()\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
    "optimizer_decod = torch.optim.Adam(decoder.parameters(), lr=0.001)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "bce_loss_fn=nn.BCELoss()   \n",
    "L2_loss_fn=nn.MSELoss()\n",
    "L1_loss_fn=nn.L1Loss()\n",
    "Complex_eye=torch.eye(int(outdim*2)).unsqueeze(1).to(device)\n",
    "\n",
    "if GELMA>0:\n",
    "    GELMA_net=M.fc_net_batch(training_data.b[0].shape[0]/2, GELMA_layers, training_data.b[0].shape[0]/2, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None, dropout=.5)\n",
    "    optimizer_GELMA = torch.optim.AdamW(GELMA_net.parameters(), lr=.001, maximize=True)\n",
    "    GELMA_net.to(device)\n",
    "    GELMA_net=nn.DataParallel(GELMA_net)\n",
    "    GELMA_net.train()\n",
    "\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_lossavg=0\n",
    "    train_lossavg=0\n",
    "    train_lossavg_decoder=0\n",
    "    inner_loss_term_avg=0\n",
    "    L1_loss_avg=0\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = True\n",
    "    for i in range(15):\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_decod.zero_grad()\n",
    "\n",
    "\n",
    "        font_size = 50\n",
    "        rho_hat=encoder(b)\n",
    "        rho_hat=sigmoid(rho_hat)\n",
    "        b_hat=decoder(rho_hat)\n",
    "        if L1_weight>0:\n",
    "            L1_loss=L1_weight*L1_loss_fn(rho_hat, rho_hat*0)\n",
    "            L1_loss.backward(retain_graph=True)  \n",
    "            L1_loss_avg+=L1_loss.item()/L1_weight/15\n",
    "            #if L1_loss>5e-15:\n",
    "                #L1_weight=L1_weight*0.999\n",
    "        else:\n",
    "            L1_loss=-1\n",
    "\n",
    "        if GELMA>0:\n",
    "            optimizer_GELMA.zero_grad()\n",
    "            GELMA_out=GELMA_net(b)\n",
    "            GELMA_inners=torch.inner(GELMA_out.squeeze(), (b-b_hat).squeeze()).diagonal(dim1=-2, dim2=-1)\n",
    "            inner_loss_term=GELMA*sum((GELMA_inners))/len(GELMA_inners)\n",
    "            inner_loss_term.backward(retain_graph=True)\n",
    "            inner_loss_term_avg+=inner_loss_term.item()/GELMA/15\n",
    "            optimizer_GELMA.step()\n",
    "        else:\n",
    "            inner_loss_term_avg=-1\n",
    "        L2_loss=L2_loss_fn(b_hat.squeeze(), b.squeeze())\n",
    "        L2_loss.backward()\n",
    "\n",
    "\n",
    "    \n",
    "        optimizer.step()\n",
    "        train_lossavg+=L2_loss.item()/15\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "    for i in range(15):\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_decod.zero_grad()\n",
    "\n",
    "        font_size = 50\n",
    "        rho_hat=encoder(b)\n",
    "        rho_hat=sigmoid(rho_hat)\n",
    "        b_hat=decoder(rho_hat)\n",
    "\n",
    "\n",
    "        \n",
    "        L2_loss=L2_loss_fn(b_hat.squeeze(), b.squeeze())\n",
    "        L2_loss.backward()\n",
    "\n",
    "\n",
    "\n",
    "        optimizer_decod.step()\n",
    "        train_lossavg_decoder+=L2_loss.item()/15\n",
    "    \n",
    "    \n",
    "    val_lossavg=0\n",
    "    if epoch%1000==0:\n",
    "       H.plot_2_imgs(rho, rho_hat,ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_decod.zero_grad()\n",
    "\n",
    "    font_size = 50\n",
    "    rho_hat=encoder(b_val)\n",
    "    rho_hat=sigmoid(rho_hat)\n",
    "    b_hat=decoder(rho_hat)\n",
    "    L2_loss=L2_loss_fn(b_hat.squeeze(), b_val.squeeze())\n",
    "    val_lossavg+=L2_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "   \n",
    "   \n",
    "    if epoch%1000==0:\n",
    "       H.plot_2_imgs(rho_val, rho_hat,ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "       \n",
    "    if epoch%100==0:\n",
    "        sum_max_inner_original=0\n",
    "        medium_hat=decoder(Complex_eye).squeeze()\n",
    "        medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "        medium_hat=medium_hat.cpu().detach().numpy()\n",
    "        medium_hat=H.cat2complex(medium_hat)                    \n",
    "        torch_inners=np.abs(np.inner(medium_hat,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "                #original way of computing inners\n",
    "                # mean over true max over hats\n",
    "        for i in range(int(outdim*2)):\n",
    "            sum_max_inner_original=sum_max_inner_original+max(torch_inners[:, i])\n",
    "            index_of_max=np.argmax(torch_inners[:,i])\n",
    "            torch_inners[:,index_of_max]=0*torch_inners[index_of_max,:]\n",
    "        max_avg_inners_original=sum_max_inner_original/(int(outdim*2))\n",
    " \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(f'epoch: {epoch}, Encoder train loss: {train_lossavg}, Decoder train loss: {train_lossavg_decoder} L1 loss: {L1_loss_avg}, val loss: {val_lossavg}, max avg inners: {max_avg_inners_original}, GELMA loss: {inner_loss_term_avg}')\n",
    "        \n",
    "    if Track_run:\n",
    "        wandb.log(wand_dict)\n",
    "\n",
    "\n",
    "if Track_run:\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cc data training\n",
    "starttime=time.time()\n",
    "target='raw'\n",
    "labeled_data=400\n",
    "layer_loss_list=[]\n",
    "Epochs=10000\n",
    "Track_run=False\n",
    "#Enter your wanbd key below and uncomment wanbd code to track run on wandb\n",
    "if Track_run:\n",
    "    key='ENTER YOUR KEY HERE' #enter your own key here\n",
    "    wandb.login(key=key) \n",
    "    wandb.init(project='Test runs')\n",
    "    wandb.config['labeled_data']=labeled_data\n",
    "wand_dict={}\n",
    "batchsize=400\n",
    "width=512\n",
    "num_layers=11\n",
    "layers=[250,1000,750,500, 500, 300, 300, 400,800]\n",
    "\n",
    "training_data=H.data_rho_CC(data_path+'/train',labeled_data/80000,1, medium='Homogenous')\n",
    "#trainloader=DataLoader(training_data,batch_size=batchsize,shuffle=True,num_workers=4)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax=nn.Softmax(dim=0)\n",
    "val_data=H.data_rho_CC(data_path+'/val', 400/80000,1)\n",
    "#valloader=DataLoader(val_data,batch_size=batchsize,shuffle=True,num_workers=4)\n",
    "\n",
    "\n",
    "encoder=M.fc_net_batch(training_data[0][0].shape[0]/2, layers, training_data.rho[0].shape[0]/4, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None)\n",
    "print('Data shapes:', training_data.b[0].shape[0]/2, training_data.rho[0].shape[0]/4)\n",
    "#Locat_orderer='/home/achristie/RM_imaging/models/CCorderer_fc_net.pt'\n",
    "#encoder.load_state_dict(torch.load(Locat_orderer))\n",
    "print(H.count_parameters(encoder))\n",
    "print('Data shapes:', training_data.b[0].shape[0], training_data.rho[0].shape[0]/2)\n",
    "encoder.to(device)\n",
    "\n",
    "\n",
    "relu=nn.ReLU()\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "bce_loss_fn=nn.CrossEntropyLoss()   \n",
    "L2_loss_fn=nn.MSELoss()\n",
    "\n",
    "\n",
    "b, rho=training_data.get_data()\n",
    "print(b.shape, rho.shape)\n",
    "b=b.to(device)\n",
    "rho=rho.to(device)\n",
    "b_val, rho_val=val_data.get_data()\n",
    "b_val=b_val.to(device)\n",
    "rho_val=rho_val.to(device)\n",
    "rh, _=torch.split(rho, 400, dim=-1)\n",
    "rh_val, _=torch.split(rho_val, 400, dim=-1)\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    train_lossavg=0\n",
    "    train_lossavg=0\n",
    "    encoder.train()\n",
    "    train_acc=0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    rho_hat=encoder(b)\n",
    "\n",
    "    bce_loss=bce_loss_fn(rho_hat.squeeze(), rh.squeeze())\n",
    "    bce_loss.backward() \n",
    "    train_lossavg=bce_loss.item()\n",
    "    optimizer.step()\n",
    "    train_acc=H.accuracy(torch.round(softmax(rho_hat.squeeze())), rh.squeeze()).item()\n",
    "    #if epoch%100==0:\n",
    "        #H.plot_2_imgs(rho, softmax(rho_hat.squeeze()),ind=5, figsize=5, xpix=20, ypix=20, font_size=25)\n",
    "    finish=time.time()-starttime\n",
    "    #print(f\"time for labeled: {finish}\")\n",
    "    val_loss=0\n",
    "    encoder.eval()\n",
    "    val_acc=0\n",
    "    font_size = 50\n",
    "    rho_hat=encoder(b_val)\n",
    "    bce_loss=bce_loss_fn(rho_hat.squeeze(), rh.squeeze())\n",
    "    val_loss+=bce_loss.item()\n",
    "    val_acc+=H.accuracy(torch.round(softmax(rho_hat.squeeze())), rho_val.squeeze()).item()\n",
    "    if epoch%100==0:\n",
    "        #H.plot_2_imgs(rho_val, softmax(rho_hat.squeeze()),ind=5, figsize=5, xpix=20, ypix=20, font_size=25)\n",
    "\n",
    "        print(f'epoch: {epoch}, train loss: {train_lossavg}, train accuracy: {train_acc} val loss: {val_loss}, val accuracy: {val_acc}')\n",
    "\n",
    "torch.save(encoder.state_dict(),f'/home/achristie/RM_imaging/models/CCorderer_fc_net.pt')\n",
    "print(f\"time for cell: {finish}\")\n",
    "\n",
    "wand_dict['val loss']=val_loss\n",
    "print(f'val loss: {val_loss}')\n",
    "if Track_run:\n",
    "    wandb.log(wand_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads orderer network, 'orderer' saved Decoder model located at 'locat'. Then reorderes the decoder columns into\n",
    "#the variable 'G_hat_permed'. Reordering is done by placing the column with the highest probability into the correct predicted position.\n",
    "#this is repeated until the entire matrix is reordered.\n",
    "layers=[250,1000,750,500, 500, 300, 300, 400,800]\n",
    "labeled_data=400\n",
    "training_data=H.data_rho_CC(data_path+'/train',labeled_data/80000,1)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "orderer=M.fc_net_batch(training_data[0][0].shape[0]/2, layers, training_data.rho[0].shape[0]/4, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None)\n",
    "print('Data shapes:', training_data[0][0].shape[0]/2, training_data.rho[0].shape[0]/2)\n",
    "Locat_orderer='/home/achristie/RM_imaging/models/CCorderer_fc_net.pt'\n",
    "#save_lcation:\"/home/achristie/Codes_data/Experiment_data/GELMA_Long/0L_160000U_[500, 500, 500, 300, 300, 400, 800]_19-May-2024 (23:36:13)\"\n",
    "orderer.load_state_dict(torch.load(Locat_orderer))\n",
    "\n",
    "layers=[500,500, 500, 300, 300, 400,800]\n",
    "training_data=H.data_rho_loaded(data_path+'/train',labeled_data/80000)\n",
    "decoder=nn.Linear(400,1450*2, bias=False)\n",
    "#locat=\"/home/achristie/Codes_data/Experiment_data/GELMA_Long/0L_40000U_[500, 500, 500, 300, 300, 400, 800]_17-May-2024 (00:41:20)decoder.pt\"\n",
    "locat='/home/achristie/Codes_data/Experiment_data/GELMA_Long/0L_160000U_[500, 500, 500, 300, 300, 400, 800]_17-May-2024 (17:39:30)decoder.pt' #.995 model\n",
    "#locat='/home/achristie/Codes_data/Experiment_data/GELMA_Long/0L_20000U_[500, 500, 500, 300, 300, 400, 800]_16-May-2024 (10:31:06)decoder.pt'\n",
    "#locat='/home/achristie/Codes_data/Experiment_data/GELMA_Long/0L_40000U_[500, 500, 500, 300, 300, 400, 800]_16-May-2024 (19:56:07)decoder.pt'\n",
    "#locat='/home/achristie/Codes_data/Experiment_data/GELMA_Long/0L_10000U_[500, 500, 500, 300, 300, 400, 800]_20-May-2024 (16:35:12)decoder.pt'\n",
    "#locat='/home/achristie/Codes_data/Experiment_data/Low_data_GELMA/0L_5000U_[500, 500, 500, 300, 300, 400, 800]_20-May-2024 (20:36:55)decoder.pt'\n",
    "decoder.load_state_dict(torch.load(locat))\n",
    "\n",
    "\n",
    "Mask=np.array(mat73.loadmat(data_path+'/M.mat')['M'])\n",
    "Complex_eye=torch.eye(400).unsqueeze(1)\n",
    "medium_hat=decoder(Complex_eye)\n",
    "medium_hat=medium_hat.squeeze()\n",
    "medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "medium_hat=medium_hat.cpu().detach().numpy()\n",
    "medium_hat=H.cat2complex(medium_hat)\n",
    "cc_medium=[]\n",
    "for i in range(len(medium_hat)):\n",
    "    outer=np.outer(medium_hat[i],medium_hat[i].conj())\n",
    "    outer=outer[abs(Mask)>0]\n",
    "    outer=outer.ravel()\n",
    "    cc_medium.append(outer)\n",
    "\n",
    "cc_medium=np.array(cc_medium)\n",
    "print(outer.shape, cc_medium.shape)\n",
    "cc_medium=torch.cat((torch.tensor(cc_medium).real, torch.tensor(cc_medium).imag), dim=-1)\n",
    "print(cc_medium.shape)\n",
    "Perm=orderer(cc_medium)\n",
    "\n",
    "softmax=nn.Softmax(dim=1)\n",
    "Perm=softmax(Perm.squeeze())\n",
    "G_hat_permed=np.empty(medium_hat.shape)+1j*np.empty(medium_hat.shape)\n",
    "T=[]\n",
    "print(Perm.shape)\n",
    "for i in range(len(Perm)):\n",
    "    largest,indices_1=torch.max(Perm, -1) #where to put the i-th element\n",
    "    overall_max_index=torch.argmax(largest)\n",
    "    G_hat_permed[indices_1[overall_max_index],:]=medium_hat[overall_max_index,:]\n",
    "    Perm[:,indices_1[overall_max_index]]=0\n",
    "\n",
    "\n",
    "print(G_hat_permed.shape, medium_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function doing above\n",
    "def reorder_step(medium_hat):\n",
    "    \n",
    "    cc_medium=[]\n",
    "    for i in range(len(medium_hat)):\n",
    "        outer=np.outer(medium_hat[i],medium_hat[i].conj())\n",
    "        outer=outer[abs(Mask)>0]\n",
    "        outer=outer.ravel()\n",
    "        cc_medium.append(outer)\n",
    "\n",
    "    cc_medium=np.array(cc_medium)\n",
    "    print(outer.shape, cc_medium.shape)\n",
    "    cc_medium=torch.cat((torch.tensor(cc_medium).real, torch.tensor(cc_medium).imag), dim=-1).float()\n",
    "    print(cc_medium.shape)\n",
    "    Perm=orderer(cc_medium)\n",
    "\n",
    "    softmax=nn.Softmax(dim=1)\n",
    "    Perm=softmax(Perm.squeeze())\n",
    "    G_hat_permed=np.empty(medium_hat.shape)+1j*np.empty(medium_hat.shape)\n",
    "    #G_hats_notremapped=[]\n",
    "    for i in range(len(Perm)):\n",
    "        largest,indices_1=torch.max(Perm, -1) #where to put the i-th element\n",
    "        overall_max_index=torch.argmax(largest)\n",
    "        G_hat_permed[indices_1[overall_max_index],:]=medium_hat[overall_max_index,:]\n",
    "        Perm[:,indices_1[overall_max_index]]=0\n",
    "\n",
    "\n",
    "    return G_hat_permed\n",
    "\n",
    "for i in range(1):\n",
    "    torch_inners=np.abs(np.inner(G_hat_permed,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "    print(sum(torch_inners.diagonal())/400)\n",
    "    G_hat_permed=reorder_step(G_hat_permed)\n",
    "print(sum(torch_inners.diagonal())/400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permutes the columns with inerse perm matrix.\n",
    "Perm=orderer(cc_medium)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "Perm=softmax(Perm.squeeze())\n",
    "hope=torch.tensor(np.linalg.inv(Perm.cpu().detach().numpy())).round()\n",
    "G_hat_permed_inver=medium_hat.squeeze().T@hope.cpu().detach().numpy().T\n",
    "G_hat_permed_inver=torch.tensor(G_hat_permed_inver.T)\n",
    "G_hat_permed_inver=F.normalize(G_hat_permed_inver, dim=-1)\n",
    "\n",
    "torch_inners=np.abs(np.inner(G_hat_permed_inver,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "print(sum(torch_inners.diagonal())/400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compares the behavior of different decoder networks trained with different random realizations and different\n",
    "#SGD randomness. \n",
    "decoder=nn.Linear(400,1450*2, bias=False)\n",
    "\n",
    "locat1='/home/achristie/Codes_data/Experiment_data/Init_opt_experiments/0L_20000U_[500, 500, 500, 300, 3000, 400, 800]_22-May-2024 (08:33:34)decoder.pt'\n",
    "locat2='/home/achristie/Codes_data/Experiment_data/Init_opt_experiments/0L_20000U_[500, 500, 500, 300, 3000, 400, 800]_22-May-2024 (01:45:38)decoder.pt'\n",
    "locat3='/home/achristie/Codes_data/Experiment_data/Init_opt_experiments/0L_20000U_[500, 500, 500, 300, 3000, 400, 800]_22-May-2024 (01:42:29)decoder.pt'\n",
    "locat4='/home/achristie/Codes_data/Experiment_data/Init_opt_experiments/0L_20000U_[500, 500, 500, 300, 3000, 400, 800]_21-May-2024 (17:34:11)decoder.pt'\n",
    "locat5='/home/achristie/Codes_data/Experiment_data/Init_opt_experiments/0L_20000U_[500, 500, 500, 300, 3000, 400, 800]_21-May-2024 (17:32:32)decoder.pt'\n",
    "\n",
    "\n",
    "locat_list=[locat1, locat2, locat3, locat4, locat5]\n",
    "locat1_indlist=[]\n",
    "locat2_indlist=[]\n",
    "locat3_indlist=[]\n",
    "locat4_indlist=[]\n",
    "locat5_indlist=[]\n",
    "total_list=[locat1_indlist, locat2_indlist, locat3_indlist, locat4_indlist, locat5_indlist]\n",
    "pmf_list=[]\n",
    "for j in range(5):\n",
    "    pmf_list.append([])\n",
    "    decoder.load_state_dict(torch.load(locat_list[j]))\n",
    "\n",
    "    Complex_eye=torch.eye(400).unsqueeze(1)\n",
    "    medium_hat=decoder(Complex_eye)\n",
    "    medium_hat=medium_hat.squeeze()\n",
    "    medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "    medium_hat=medium_hat.cpu().detach().numpy()\n",
    "    medium_hat=H.cat2complex(medium_hat)\n",
    "    torch_inners=np.abs(np.inner(medium_hat,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j)\n",
    "    #pmf_list.append(torch_inners.diagonal)\n",
    "    larger_than=0\n",
    "    sum_max_noreplace_original=0\n",
    "    dx=.05\n",
    "\n",
    "    for k in range(int(1/dx)):\n",
    "        \n",
    "        for i in range(400):\n",
    "            if max(torch_inners[i,:])>k*dx and max(torch_inners[i,:])<(k+1)*dx:\n",
    "                larger_than+=1\n",
    "            if dx*j>=95 and i not in total_list[j]:\n",
    "                total_list[j].append(i)\n",
    "            sum_max_noreplace_original=sum_max_noreplace_original+max(torch_inners[:,i]) #Fix g, find max over ghat\n",
    "        #print(larger_than)   \n",
    "        pmf_list[j].append(larger_than/400)\n",
    "\n",
    "\n",
    "\n",
    "j = 0  # replace with your desired index\n",
    "\n",
    "# Ensure pmf_list[j] is not empty\n",
    "if pmf_list[j]:\n",
    "    plt.bar([dx*j for j in range(1,21)], pmf_list[j])\n",
    "    plt.title(f'PMF for index {j}')\n",
    "    plt.xlabel('Outcome')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"No data available for index {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indices_used=[i for j in total_list for i in j]\n",
    "indices_used=list(set(indices_used))\n",
    "shared_inds=list(set(locat1_indlist).intersection(locat2_indlist))\n",
    "len(shared_inds), ([len(locat1_indlist), len(locat2_indlist)]), len(indices_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Tests orderer on fully random data  \n",
    "layers=[250,1000,750,500, 500, 300, 300, 400,800]\n",
    "labeled_data=400\n",
    "data_path_PNAS=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "data_path_MULTI_REAL=os.path.join(cwd,'Data/MDS-PNAS-regime-1000real_all_seeds/MDS-PNAS-regime-1000real_seed0') \n",
    "\n",
    "#original_G_list= np.array(mat73.loadmat(data_path_MULTI_REAL+'/rtt_array.mat')['RM_realization_list'])\n",
    "test_g=original_G_list[2]\n",
    "print(original_G_list.shape)\n",
    "\n",
    "\n",
    "\n",
    "training_data=H.data_rho_CC(data_path_PNAS+'/train',labeled_data/80000,1)\n",
    "\n",
    "medium_index=0\n",
    "\n",
    "medium= np.array(mat73.loadmat(data_path_PNAS+'/rtt.mat')['Artt'])\n",
    "medium=test_g\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "softmax=nn.Softmax(dim=1)\n",
    "orderer=M.fc_net_batch(training_data[0][0].shape[0]/2, layers, training_data.rho[0].shape[0]/4, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None)\n",
    "print('Data shapes:', training_data[0][0].shape[0]/2, training_data.rho[0].shape[0]/2)\n",
    "Locat_orderer='/home/achristie/RM_imaging/models/CCorderer_fc_net.pt'\n",
    "orderer.load_state_dict(torch.load(Locat_orderer))\n",
    "orderer.to(device)\n",
    "training_data=H.data_rho_CC_IID(data_path_MULTI_REAL+'/train',labeled_data/80000)\n",
    "training_data.b=medium.T\n",
    "rho=np.identity(len(medium.T))+j*np.zeros((len(medium.T),len(medium.T)))\n",
    "rho=torch.cat((torch.tensor(rho.real),torch.tensor(rho.imag)),dim=-1).float()\n",
    "training_data.rho=rho\n",
    "rho=rho.detach().numpy()\n",
    "\n",
    "rho=H.cat2complex(rho)\n",
    "b=training_data.b   \n",
    "print(b.shape, rho.shape, medium.shape)\n",
    "if not np.allclose(medium@rho.T, b.T):\n",
    "    print('data misformatted')\n",
    "\n",
    "\n",
    "trainloader=DataLoader(training_data,batch_size=400,shuffle=True,num_workers=0)\n",
    "orderer.eval()\n",
    "accuracy=0\n",
    "lossavg=0\n",
    "bce_loss_fn=nn.CrossEntropyLoss()   \n",
    "for batch, (b,rho,num_targets) in enumerate(trainloader):\n",
    "\n",
    "    b=b.to(device)\n",
    "    rho=rho.to(device)    \n",
    "    rho_hat=orderer(b)\n",
    "    rh, _=torch.split(rho, 400, dim=-1)\n",
    "    bce_loss=bce_loss_fn(rho_hat.squeeze(), rh.squeeze())\n",
    "    lossavg+=bce_loss.item()/len(trainloader)\n",
    "    H.plot_2_imgs(rho, softmax(rho_hat.squeeze()),ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "    accuracy+=H.accuracy(torch.round(softmax(rho_hat.squeeze())), rh.squeeze()).item()/len(trainloader)\n",
    "\n",
    "print(f'accuracy: {accuracy}, loss: {lossavg}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Tests if just looking at  proximoity of random media, and homo media data.\n",
    "labeled_data=1000\n",
    "data_path=os.path.join(cwd,'Data/MDS-PNAS-regime-1000real_all_seeds/MDS-PNAS-regime-1000real_seed0') \n",
    "training_data=H.data_rho_CC(data_path+'/train',labeled_data/80000,1)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "test_cc=training_data[0][0]\n",
    "target=training_data[0][1]\n",
    "dummies=[]\n",
    "print(medium.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Tests ordered on fully random data  \n",
    "layers=[250,1000,750,500, 500, 300, 300, 400,800]\n",
    "data_path=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "labeled_data=400\n",
    "training_data=H.data_rho_CC(data_path+'/train',labeled_data/80000,1)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "softmax=nn.Softmax(dim=1)\n",
    "\n",
    "orderer=M.fc_net_batch(training_data[0][0].shape[0]/2, layers, training_data.rho[0].shape[0]/4, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None)\n",
    "print('Data shapes:', training_data[0][0].shape[0]/2, training_data.rho[0].shape[0]/2)\n",
    "Locat_orderer='/home/achristie/RM_imaging/models/CCorderer_fc_net.pt'\n",
    "orderer.load_state_dict(torch.load(Locat_orderer))\n",
    "data_path=os.path.join(cwd,'Data/MDS-PNAS-regime-1000real_all_seeds/MDS-PNAS-regime-1000real_seed0') \n",
    "\n",
    "training_data=H.data_rho_CC_IID(data_path+'/train',labeled_data/80000)\n",
    "orderer.to(device)\n",
    "trainloader=DataLoader(training_data,batch_size=400,shuffle=True,num_workers=0)\n",
    "orderer.eval()\n",
    "accuracy=0\n",
    "lossavg=0\n",
    "bce_loss_fn=nn.CrossEntropyLoss()   \n",
    "for batch, (b,rho,num_targets) in enumerate(trainloader):\n",
    "    \n",
    "    b=b.to(device)  \n",
    "    rho=rho.to(device)    \n",
    "    rho_hat=orderer(b)\n",
    "    rh, _=torch.split(rho, 400, dim=-1)\n",
    "    bce_loss=bce_loss_fn(rho_hat.squeeze(), rh.squeeze())\n",
    "    lossavg+=bce_loss.item()/len(trainloader)\n",
    "    H.plot_2_imgs(rho, softmax(rho_hat.squeeze()),ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "    curr_acc=H.accuracy(torch.round(softmax(rho_hat.squeeze())), rh.squeeze()).item()\n",
    "    if curr_acc<1.0:\n",
    "        print(curr_acc)\n",
    "    accuracy+=H.accuracy(torch.round(softmax(rho_hat.squeeze())), rh.squeeze()).item()/len(trainloader)\n",
    "    \n",
    "\n",
    "print(f'accuracy: {accuracy}, loss: {lossavg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "medium1= np.array(mat73.loadmat(data_path+'/G_0.mat')['A0'])\n",
    "data_path=os.path.join(cwd,'Data/MDS-PNAS-regime-1000real_all_seeds/MDS-PNAS-regime-1000real_seed0')\n",
    "medium2= np.array(mat73.loadmat(data_path+'/G_0.mat')['A0'])\n",
    "np.allclose(medium1,medium2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests model with 2 scatters\n",
    "data_path=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "training_data=H.data_rho_CC(data_path+'/train',labeled_data/80000,1)\n",
    "b=training_data.b[:25]\n",
    "rho=training_data.rho[:25]\n",
    "b=b+training_data.b[25:50]\n",
    "#b=b.T/(np.sum((abs(b)**2), -1)**(1/2))\n",
    "#b=b.T\n",
    "rho=rho+training_data.rho[25:50]\n",
    "training_data.b=b\n",
    "training_data.rho=rho\n",
    "\n",
    "trainloader=DataLoader(training_data,batch_size=400,shuffle=True,num_workers=0)\n",
    "orderer.eval()\n",
    "accuracy=0\n",
    "lossavg=0\n",
    "bce_loss_fn=nn.CrossEntropyLoss()   \n",
    "for batch, (b,rho,num_targets) in enumerate(trainloader):\n",
    "    \n",
    "    b=b.to(device)\n",
    "    rho=rho.to(device)    \n",
    "    rho_hat=orderer(b)\n",
    "    rh, _=torch.split(rho, 400, dim=-1)\n",
    "    bce_loss=bce_loss_fn(rho_hat.squeeze(), rh.squeeze())\n",
    "    lossavg+=bce_loss.item()/len(trainloader)\n",
    "    print(torch.max(softmax(rho_hat.squeeze())))\n",
    "    H.plot_2_imgs(rho, softmax(rho_hat.squeeze()),ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "    \n",
    "    accuracy+=H.accuracy(torch.round(softmax(rho_hat.squeeze())), rh.squeeze()).item()/len(trainloader)\n",
    "\n",
    "print(f'accuracy: {accuracy}, loss: {lossavg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path=os.path.join(cwd,'Data/MDS-PNAS-regime-1000real_all_seeds/MDS-PNAS-regime-1000real_seed0') \n",
    "b=np.load(data_path+'/train/b.npy')\n",
    "b.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester=b[0]\n",
    "sum(abs(tester)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plots the ordered decoder and the original KM image\n",
    "\n",
    "torch_inners=np.abs(np.inner(G_hat_permed,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "print(sum(torch_inners.diagonal())/400)\n",
    "\n",
    "def KM_img_hat(rho, sensing_hat,sensing,figsize=8,scaling='Linf',font_size=50, file_name=None, xpix=31, ypix=21,WAND=False):\n",
    "    plt.close()\n",
    "\n",
    "    b=sensing@rho.T\n",
    "    img=sensing_hat.T.conj()@b\n",
    "    \n",
    "    figsize=(figsize,figsize)\n",
    "    fig=plt.figure(figsize=figsize)\n",
    "\n",
    "    tick_params = {'labelsize': font_size}\n",
    "\n",
    "\n",
    "\n",
    "    output=img\n",
    "\n",
    "    output=np.abs(output)/np.max(np.abs((output)))\n",
    "    \n",
    "    output=output.reshape(xpix, ypix)\n",
    "\n",
    "    ax=plt.gca()\n",
    "    pcol2=ax.pcolor(output,cmap='jet')\n",
    "    cbar=plt.colorbar(pcol2, ax=ax)\n",
    "    \n",
    "    cbar.ax.tick_params(labelsize=font_size)\n",
    "    cbar.remove()\n",
    "    ax.tick_params(axis='both', **tick_params)\n",
    "    if file_name!=None:\n",
    "        plt.savefig(f'/home/achristie/Codes_data/E_D_figs/{file_name}.pdf')\n",
    "    plt.tight_layout()\n",
    "    if WAND:\n",
    "        \n",
    "        \n",
    "        buf = io.BytesIO()\n",
    "        fig.savefig(buf)\n",
    "        buf.seek(0)\n",
    "        img = Image.open(buf)\n",
    "        \n",
    "        del fig, buf\n",
    "        return [wandb.Image(img)]\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(medium.shape)\n",
    "rho=np.zeros((400))\n",
    "rho[210]=1\n",
    "#rho[0]=1\n",
    "\n",
    "KM_img_hat(rho, G_hat_permed.T, medium,xpix=20, ypix=20, file_name='rho_hat_KM')\n",
    "KM_img_hat(rho, medium, medium,xpix=20, ypix=20, file_name='rho_KM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch_inners=np.inner(medium_hat,medium.T.conj())\n",
    "torch_inners=np.abs(torch_inners)\n",
    "larger_than=0\n",
    "sum_max_noreplace_original=0\n",
    "for i in range(400):\n",
    "    if max(torch_inners[i,:])>.95:\n",
    "        larger_than+=1\n",
    "    sum_max_noreplace_original=sum_max_noreplace_original+max(torch_inners[:,i]) #Fix g, find max over ghat\n",
    "\n",
    "avg_max_inners_original=sum_max_noreplace_original/400\n",
    "larger_than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stupid=torch.tensor([[8,5],[6,7]]).float()\n",
    "print(stupid.shape)\n",
    "torch.max(stupid, -1), softmax(stupid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_inners=np.inner(G_hat_permed,medium.T.conj()).diagonal()\n",
    "torch_inners_abs=np.abs(np.inner(G_hat_permed,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "print(sum(torch_inners_abs.diagonal())/400)\n",
    "Inners=torch_inners_abs.diagonal()\n",
    "print(np.mean(Inners[Inners<.90]))\n",
    "print(len(torch_inners[torch_inners.imag<0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MINI_g_hat_permed=G_hat_permed[Inners<.90]\n",
    "\n",
    "Mini_medium=medium.T[Inners<.90]\n",
    "mini_torch_inners=np.inner(MINI_g_hat_permed,Mini_medium.conj())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_inners=np.inner(medium_hat,medium.T.conj())\n",
    "torch_inners=mini_torch_inners\n",
    "torch_inners=np.abs(torch_inners)\n",
    "print(torch_inners[0][0], np.inner(medium_hat,medium.T.conj())[0][0])\n",
    "sum_max_noreplace_original=0\n",
    "for i in range(len(MINI_g_hat_permed)):\n",
    "    sum_max_noreplace_original=sum_max_noreplace_original+max(torch_inners[:,i]) #Fix g, find max over ghat\n",
    "\n",
    "avg_max_inners_original=sum_max_noreplace_original/len(MINI_g_hat_permed)\n",
    "avg_max_inners_original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads model from location and gets Ghat error\n",
    "data_path='/home/achristie/Codes_data/PNAS-regime_all_seeds/PNAS-regime_seed0'\n",
    "location='/home/achristie/Codes_data/Experiment_data/Complex_tests/0L_320000U_[500, 500, 500, 300, 300, 400, 800]_06-May-2024 (14:12:11)decoder.pt'\n",
    "decoder=nn.Linear(400, 1450*2, bias=False)\n",
    "decoder.load_state_dict(torch.load(location))\n",
    "Complex_eye=torch.eye(400).unsqueeze(1)\n",
    "medium_hat=decoder(Complex_eye).squeeze()\n",
    "medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "medium_hat=medium_hat.cpu().detach().numpy()\n",
    "medium_hat=H.cat2complex(medium_hat)\n",
    "torch_inners=np.abs(np.inner(medium_hat,medium.T.conj(), ).diagonal())\n",
    "mean_inn_product=np.mean(torch_inners)\n",
    "print(mean_inn_product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=[500,500, 500, 300, 300, 400,800]\n",
    "softmax=nn.Softmax(dim=1)\n",
    "Orderer=M.fc_net_extra(training_data.b[0].shape[0]/2, layers, training_data.rho[0].shape[0]/4, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None)\n",
    "locat=\"/home/achristie/Codes_data/Experiment_data/hom/5000L_0U_[500, 500, 500, 300, 300, 400, 800]_13-May-2024 (13:14:58)\"\n",
    "Orderer.load_state_dict(torch.load(locat+'encoder.pt'))\n",
    "decoder_locat=\"/home/achristie/Codes_data/Experiment_data/Long_best_exps/0L_320000U_[500, 500, 500, 300, 300, 400, 800]_12-May-2024 (11:03:02)decoder.pt\"\n",
    "Orderer.eval()\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "decoder=nn.Linear(400, 1450*2, bias=False)\n",
    "decoder.load_state_dict(torch.load(decoder_locat))\n",
    "decoder_weights=decoder.weight.cpu().detach().numpy().T\n",
    "decoder_weights=torch.tensor(decoder_weights)\n",
    "decoder_weights=F.normalize(decoder_weights, dim=-1)\n",
    "perm=Orderer(decoder_weights)\n",
    "\n",
    "#gotta confirm outscaling is the same\n",
    "perm=(softmax(perm))\n",
    "largest,indices=torch.max(perm, -1)\n",
    "perm_mat=torch.zeros(perm.shape)\n",
    "for i in range(len(perm)):\n",
    "    perm_mat[i, indices[i]]=1\n",
    "\n",
    "perm_mat=perm_mat.cpu().detach().numpy()\n",
    "medium_hat=H.cat2complex(decoder_weights.cpu().detach().numpy())\n",
    "medium_hat_permed=(medium_hat.T@np.linalge.inv(perm_mat)).T\n",
    "medium_hat_permed=F.normalize(torch.tensor(medium_hat_permed), dim=-1).numpy()\n",
    "#medium_hat_inverse_permed=(medium_hat.T@np.linalg.inv(perm_mat.T)).T\n",
    "\n",
    "\n",
    "\n",
    "max_inners_1=0\n",
    "torch_inners=np.abs(np.inner(medium_hat_permed.squeeze(),medium.T.conj()))\n",
    "for i in range(int(200*2)):\n",
    "    max_inners_1=max_inners_1+max(torch_inners[:,i]) #Fix g, find max over ghat\n",
    "\n",
    "max_inners_1=max_inners_1/(int(200*2))\n",
    "\n",
    "\n",
    "print(max_inners_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.array(mat73.loadmat(data_path+'/raw_train.mat')['b_d_train'])\n",
    "rho=np.array(mat73.loadmat(data_path+'/rho_train.mat')['rho_d_train'])\n",
    "cc=np.array(mat73.loadmat(data_path+'/cc_train.mat')['cc_d_train'])\n",
    "Mask=np.array(mat73.loadmat(data_path+'/M.mat')['M'])\n",
    "\n",
    "outer=np.outer(b,b.conj())\n",
    "outer=outer[abs(Mask)>0]\n",
    "outer=outer.ravel()\n",
    "\n",
    "\n",
    "\n",
    "        #self.Mask=np.array(mat73.loadmat(data_path+'/M.mat')['M'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(outer[0].real, cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outer.shape, cc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(outer,cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path\n",
    "self.b=np.load(data_path+'/b.npy') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_rho_CC:\n",
    "    def __init__(self,data_path ,prop,sparsity=4):\n",
    "        if 'PNAS' in data_path and \"train\" in data_path:\n",
    "            self.rho, self.b=H.Generate_data_pnas(data_path[:-5],int(80000*prop), S=sparsity,seed=0)\n",
    "            self.data_path=data_path[:-5]\n",
    "\n",
    "        elif 'PNAS' in data_path and 'val' in data_path:\n",
    "            self.rho, self.b=H.Generate_data_pnas(data_path[:-3],3000, S=sparsity,seed=100)\n",
    "            self.data_path=data_path[:-3]\n",
    "        self.Mask=np.array(mat73.loadmat(self.data_path+'/M.mat')['M'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return(int(len(self.b)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        outer=np.outer(self.b[idx,...],self.b[idx,...].conj())\n",
    "        outer=outer[abs(self.Mask)>0]\n",
    "        outer=outer.ravel()\n",
    "\n",
    "        return torch.cat((torch.tensor(outer.real),torch.tensor(outer.imag)),dim=-1).float(),self.rho[idx,...].float(), float(torch.sum(self.rho[idx,...].real))\n",
    "        \n",
    "training_data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=H.data_rho_CC(data_path+'/train',500/80000,1)\n",
    "b=np.array(mat73.loadmat(data_path+'/raw_train.mat')['b_d_train'])\n",
    "rho=np.array(mat73.loadmat(data_path+'/rho_train.mat')['rho_d_train'])\n",
    "cc=np.array(mat73.loadmat(data_path+'/cc_train.mat')['cc_d_train'])\n",
    "Mask=np.array(mat73.loadmat(data_path+'/M.mat')['M'])\n",
    "b=training_data.b[0]\n",
    "outer=np.outer(b,b.conj())\n",
    "outer=outer[abs(Mask)>0]\n",
    "outer=outer.ravel()\n",
    "outer[5],training_data[0][0][5].numpy().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reppeated=np.sort(indices.numpy())\n",
    "dum=[]\n",
    "num=0\n",
    "for i in range(len(reppeated)):\n",
    "    \n",
    "    if reppeated[i] not in dum:\n",
    "        dum.append(reppeated[i])\n",
    "    else:\n",
    "        num=num+1\n",
    "num  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_inners=np.abs(np.inner(G_hat_permed,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "torch_inners=torch_inners[ghat_indices,:]\n",
    "#sum(torch_inners.diagonal()[torch_inners.diagonal()<.99])/29\n",
    "\n",
    "sum_max_noreplace_original=0\n",
    "for i in range(29):\n",
    "    sum_max_noreplace_original=sum_max_noreplace_original+max(torch_inners[i,:]) #Fix g, find max over ghat\n",
    "sum_max_noreplace_original/29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_inners=np.abs(np.inner(G_hat_permed,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "ghat_indices=[]\n",
    "bad_inners=0\n",
    "pmf=[]\n",
    "for i in range(len(torch_inners)):\n",
    "    if torch_inners[i,i]<.99:\n",
    "        ghat_indices.append(i)\n",
    "        print(i, torch_inners[i,i])\n",
    "        bad_inners=bad_inners+torch_inners[i,i]\n",
    "print(bad_inners/29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cc data training\n",
    "starttime=time.time()\n",
    "target='raw'\n",
    "labeled_data=400\n",
    "layer_loss_list=[]\n",
    "Epochs=2000\n",
    "Track_run=False\n",
    "#Enter your wanbd key below and uncomment wanbd code to track run on wandb\n",
    "if Track_run:\n",
    "    key='ENTER YOUR KEY HERE' #enter your own key here\n",
    "    wandb.login(key=key) \n",
    "    wandb.init(project='Test runs')\n",
    "    wandb.config['labeled_data']=labeled_data\n",
    "wand_dict={}\n",
    "batchsize=400\n",
    "layers=[250,1000,750,500, 500, 300, 300, 400,800]_\n",
    "training_data=H.data_rhoCC(data_path+'/train',labeled_data/80000,1)\n",
    "trainloader=DataLoader(training_data,batch_size=batchsize,shuffle=True,num_workers=4)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax=nn.Softmax(dim=0)\n",
    "val_data=H.data_rho_CC(data_path+'/val', 3000/80000,1)\n",
    "valloader=DataLoader(val_data,batch_size=batchsize,shuffle=True,num_workers=4)\n",
    "\n",
    "\n",
    "encoder=M.fc_net_batch(training_data[0][0].shape[0]/2, layers, training_data.rho[0].shape[0]/4, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None)\n",
    "print('Data shapes:', training_data.b[0].shape[0]/2, training_data.rho[0].shape[0]/4)\n",
    "Locat_orderer='/home/achristie/RM_imaging/models/CCorderer_fc_net.pt'\n",
    "encoder.load_state_dict(torch.load(Locat_orderer))\n",
    "print(H.count_parameters(encoder))\n",
    "print('Data shapes:', training_data.b[0].shape[0]/2, training_data.rho[0].shape[0]/4)\n",
    "encoder.to(device)\n",
    "\n",
    "\n",
    "relu=nn.ReLU()\n",
    "optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "bce_loss_fn=nn.CrossEntropyLoss()   \n",
    "L2_loss_fn=nn.MSELoss()\n",
    "for epoch in range(Epochs):\n",
    "    train_lossavg=0\n",
    "    train_lossavg=0\n",
    "    encoder.train()\n",
    "    for batch, (b,rho,num_targets) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        b=b.to(device)\n",
    "        rho=rho.to(device)    \n",
    "        rho_hat=encoder(b)\n",
    "        rh, _=torch.split(rho, 400, dim=-1)\n",
    "        bce_loss=bce_loss_fn(rho_hat.squeeze(), rh.squeeze())\n",
    "        bce_loss.backward() \n",
    "        train_lossavg+=bce_loss.item()/len(trainloader)\n",
    "        optimizer.step()\n",
    "    if epoch==Epochs-1:\n",
    "        H.plot_2_imgs(rho, softmax(rho_hat),ind=5, figsize=5, scaling=None, xpix=20, ypix=20, font_size=25)\n",
    "        finish=time.time()-starttime\n",
    "        print(f\"time for labeled: {finish}\")\n",
    "    print(f'epoch: {epoch}, train loss: {train_lossavg}')\n",
    "val_loss=0\n",
    "encoder.eval()\n",
    "for batch, (b,rho,num_targets) in enumerate(valloader):\n",
    "    b=b.to(device)\n",
    "    rho=rho.to(device)    \n",
    "    font_size = 50\n",
    "    rho_hat=encoder(b)\n",
    "    rh, _=torch.split(rho, 400, dim=-1)\n",
    "    bce_loss=bce_loss_fn(rho_hat.squeeze(), rh.squeeze())\n",
    "    val_loss+=bce_loss.item()/len(valloader)\n",
    "    if batch==0:\n",
    "        H.plot_2_imgs(rho, softmax(rho_hat),ind=5, figsize=5, xpix=20, ypix=20, font_size=25)\n",
    "\n",
    "#torch.save(encoder.state_dict(),f'/home/achristie/RM_imaging/models/CCorderer_fc_net.pt')\n",
    "\n",
    "wand_dict['val loss']=val_loss\n",
    "print(f'val loss: {val_loss}')\n",
    "if Track_run:\n",
    "    wandb.log(wand_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=nn.Linear(400,1450*2, bias=False)\n",
    "data_path=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "\n",
    "#locat=\"/home/achristie/Codes_data/Experiment_data/GELMA_Long/0L_40000U_[500, 500, 500, 300, 300, 400, 800]_17-May-2024 (00:41:20)decoder.pt\"\n",
    "locat='/home/achristie/Codes_data/Experiment_data/GELMA_Long/0L_160000U_[500, 500, 500, 300, 300, 400, 800]_17-May-2024 (17:39:30)decoder.pt' #.995 model\n",
    "decoder.load_state_dict(torch.load(locat))\n",
    "\n",
    "Complex_eye=torch.eye(400).unsqueeze(1)\n",
    "medium_hat=decoder(Complex_eye)\n",
    "medium_hat=medium_hat.squeeze()\n",
    "medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "medium_hat=medium_hat.cpu().detach().numpy()\n",
    "medium_hat=H.cat2complex(medium_hat)\n",
    "torch_inners=np.abs(np.inner(medium_hat,medium_hat.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "medium_hat.shape, torch_inners.shape\n",
    "\n",
    "sum_max_inner_original=0\n",
    "medium_hat=decoder(Complex_eye).squeeze()\n",
    "medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "medium_hat=medium_hat.cpu().detach().numpy()\n",
    "medium_hat=H.cat2complex(medium_hat)                    \n",
    "torch_inners=(np.inner(medium_hat,medium.T.conj()).real) #entry i j is ghat_i dot bar(g_j) \n",
    "        #original way of computing inners\n",
    "        # mean over true max over hats\n",
    "for i in range(int(200*2)):\n",
    "    sum_max_inner_original=sum_max_inner_original+max(torch_inners[:, i])\n",
    "sum_max_inner_original/400\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

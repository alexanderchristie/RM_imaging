{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebooks for small tests\n",
    "import os  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "plt.rcParams['axes.facecolor']='w'\n",
    "plt.rcParams['savefig.facecolor']='w'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "#imports and plotting function\n",
    "import argparse\n",
    "import mat73\n",
    "import logging\n",
    "import numpy as np\n",
    "#import torchvision\n",
    "from datetime import datetime\n",
    "import os\n",
    "import scipy.io\n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import Models as M\n",
    "import Helpers as H\n",
    "import time\n",
    "import copy\n",
    "from torch.func import vmap\n",
    "from functorch.experimental import replace_all_batch_norm_modules_\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "cwd=os. getcwd()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path=os.path.join(cwd,'Data/FoldyLox_all_seeds/FoldyLox_seed0')\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "print(medium.shape)\n",
    "inners=medium.transpose().conjugate()@medium\n",
    "print(inners.shape)\n",
    "coherence=0\n",
    "for i in inners:\n",
    "    for j in i:\n",
    "        if abs(j)>coherence and j<.99:\n",
    "            coherence=abs(j)\n",
    "print('coherence of data: ', coherence)\n",
    "#b=np.load(data_path+'/train/b.npy')\n",
    "#rho=np.load(data_path+'/train/rho.npy')\n",
    "#print(np.allclose(medium@rho[0],b[0]))\n",
    "index_list=[]\n",
    "unlabeled_data=5000\n",
    "MIN_CENTERS_LIST=[5] #Min number of centers for DBSCAN\n",
    "eps_linspace=np.linspace(.00005,.03,50) #Epsilon values for DBSCAN\n",
    "\n",
    "\n",
    "decoder_locats='/home/achristie/Codes_data/Experiment_data/rhosupport_stats/Scaling_tests/10kepochs/decoder_5000_'\n",
    "timescampstrs = [\n",
    "\"44_00-31-12\",\n",
    "\"47_00-31-10\",\n",
    "\"43_18-10-40\",\n",
    "\"46_18-10-39\",\n",
    "\"41_18-10-04\",\n",
    "\"49_18-00-09\",\n",
    "\"48_11-52-40\",\n",
    "\"45_11-50-38\",\n",
    "\"42_11-50-21\",\n",
    "\"40_11-49-46\",\n",
    "\"25_8-6-24\",\n",
    "\"24_01-54-27\",\n",
    "\"9_21-43-17\",\n",
    "\"4_21-42-58\",\n",
    "\"23_17-55-08\",\n",
    "\"29_15-29-37\",\n",
    "\"39_15-11-34\",\n",
    "\"34_15-09-31\",\n",
    "\"8_13-43-00\",\n",
    "\"3_13-42-41\",\n",
    "\"22_09-55-47\",\n",
    "\"38_08-50-47\",\n",
    "\"33_08-47-54\",\n",
    "\"28_08-46-30\",\n",
    "\"7_05-43-01\",\n",
    "\"2_05-42-47\",\n",
    "\"37_02-29-47\",\n",
    "\"32_2-26-51\",\n",
    "\"27_2-25-22\",\n",
    "\"21_01-56-27\",\n",
    "\"6_21-43-02\",\n",
    "\"1_21-42-47\",\n",
    "\"36_20-08-25\",\n",
    "\"31_20-05-29\",\n",
    "\"26_20-04-14\",\n",
    "\"20_17-57-14\",\n",
    "\"35_13-47-33\",\n",
    "\"30_13-44-32\",\n",
    "\"5_13-43-10\",\n",
    "\"0_13-42-53\",\n",
    "\"19_11-58-46\",\n",
    "\"18_8-27-31\",\n",
    "\"17_5-3-27\",\n",
    "\"16_01-39-09\",\n",
    "\"15_22-14-51\",\n",
    "\"14_17-34-09\",\n",
    "\"13_12-36-21\",\n",
    "\"12_9-12-21\",\n",
    "\"11_05-47-25\",\n",
    "\"10_1-9-58\"\n",
    "]\n",
    "\n",
    "#,'21-52-00','01-44-08','05-09-23','01-48-20', '09-27-26','05-27-23','08-57-23','03-35-41'}\n",
    "\n",
    "\n",
    "pixels='Gaussian_abs'\n",
    "T_hold=.99\n",
    "def oreinted_mean(z):\n",
    "    z_inners=np.inner(z,z.conj())\n",
    "    ret_z=[]\n",
    "    for  i in range(z.shape[0]):\n",
    "        if z_inners[0,i]>0:\n",
    "            ret_z.append(z[i])\n",
    "        else:\n",
    "            ret_z.append(-z[i])\n",
    "    return np.mean(np.array(ret_z), axis=0)\n",
    "decoder_locats=[decoder_locats+timescampstr+'.pt' for timescampstr in timescampstrs]\n",
    "training_data=H.data_rho_loaded(data_path+'/train',unlabeled_data/80000,pixels=pixels, normalize=False)\n",
    "indim=int(training_data.b[0].shape[0]/2)\n",
    "outdim=(training_data.rho[0].shape[0]/4)\n",
    "def get_cluster_data(decoder_locats):\n",
    "    Complex_eye=torch.cat((torch.eye(int(outdim*2)), torch.zeros((int(outdim*2),int(outdim*2)))), dim=1)\n",
    "\n",
    "    #Complex_eye=torch.eye(400)#.to(device)\n",
    "    columns=[]\n",
    "    \n",
    "    #decoder_locats=f'/home/achristie/Codes_data/Experiment_data/rhosupport_stats/5khighcohdatamodels/'\n",
    "    #decoder_locats = '/home/achristie/Codes_data/Experiment_data/rhosupport_stats/5khighcohdatamodels/'\n",
    "    #decoder_files = [f for f in os.listdir(decoder_locats) if f.startswith('decoder')]\n",
    "    for curr_decoder in decoder_locats:\n",
    "        decoder=M.norm_linear_complex(int(training_data.rho[0].shape[0]), int(training_data.b[0].shape[0]), normalize=False)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "\n",
    "        #decoder=M.norm_linear_complex(int(training_data.rho[0].shape[0]/2), int(training_data.b[0].shape[0]), normalize=False)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "        try:\n",
    "            decoder.load_state_dict(torch.load(curr_decoder))\n",
    "        except:\n",
    "            #curr_decoder='/home/achristie/Codes_data/Experiment_data/rhosupport_stats/Classic_gelma/decoder_5000_1'+curr_decoder[-12:]\n",
    "            #decoder.load_state_dict(torch.load(curr_decoder))\n",
    "            pass\n",
    "        medium_hat=decoder(Complex_eye).squeeze()\n",
    "        medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "        medium_hat=medium_hat.cpu().detach().numpy()\n",
    "        medium_hat=H.cat2complex(medium_hat)                    \n",
    "        columns.append(medium_hat)\n",
    "    columns=np.array(columns)\n",
    "    return  np.row_stack(columns)\n",
    "\n",
    "\n",
    "col_list=get_cluster_data(decoder_locats)\n",
    "print(f'shape of the union without removin: {col_list.shape}, number for Expceriments: {len(col_list)//361}')\n",
    "for i in range(361):\n",
    "    G_i=medium.T[i]\n",
    "    print(G_i.shape)\n",
    "    break\n",
    "\n",
    "def db_inner(a,b):\n",
    "    a=H.cat2complex(a)\n",
    "    b=H.cat2complex(b)\n",
    "    return 1-abs(np.inner(a,b.conj()))\n",
    "\n",
    "X=col_list\n",
    "X = np.concatenate([X.real, X.imag], axis=1)\n",
    "columns=X\n",
    "def DBSCAN_inners_fig(col_list, min_centers):\n",
    "    X=col_list\n",
    "    X = np.concatenate([X.real, X.imag], axis=1)\n",
    "    num_clusters=[]\n",
    "    ind_not_recovered_list=[]\n",
    "    torch_inner=np.abs(1.0-(np.abs(np.inner(col_list,col_list.conj()))))\n",
    "    num_recovered_list=[]\n",
    "    num_recovered_list_97=[]\n",
    "    Cluster_size_diff_list=[]\n",
    "    avg_cluster_size=[]\n",
    "    smallest_cluster_used=[]\n",
    "    next_largest_cluster=[]\n",
    "    eps_list=[]\n",
    "    num_really_bad=[]\n",
    "    num_recovered_list_CORES=[]\n",
    "\n",
    "\n",
    "    for eps in eps_linspace:\n",
    "        DBSCAN_fit=DBSCAN(eps=eps, min_samples=min_centers, metric='precomputed').fit(torch_inner)\n",
    "        DBSCAN_fit.labels_\n",
    "        X_clustered=X[DBSCAN_fit.labels_!=-1]\n",
    "        len(X_clustered)\n",
    "        cluster_sizes=[len(X[DBSCAN_fit.labels_==i]) for i in range(max(DBSCAN_fit.labels_))]\n",
    "        sorted_cluster_sizes=sorted(cluster_sizes)\n",
    "        try:\n",
    "            min_accepted_cluster_size=sorted_cluster_sizes[-361]\n",
    "        except:\n",
    "            min_accepted_cluster_size=0\n",
    "            #Cluster_size_diff_list.append(0)\n",
    "        if len(cluster_sizes)>0:\n",
    "            ghat_list_cores=[]\n",
    "            ghat_list_avgs=[]\n",
    "\n",
    "            for i in range(max(DBSCAN_fit.labels_)):\n",
    "                \n",
    "                if cluster_sizes[i]>=min_accepted_cluster_size:\n",
    "                    cluster_cores=[k for k in range(len(X)) if DBSCAN_fit.labels_[k]==i and k in DBSCAN_fit.core_sample_indices_]\n",
    "                    ghat_list_cores.append(oreinted_mean(X[cluster_cores]))\n",
    "                    ghat_list_avgs.append(oreinted_mean(X[DBSCAN_fit.labels_==i]))\n",
    "                    #if np.allclose(oreinted_mean(X[cluster_cores]),oreinted_mean(X[DBSCAN_fit.labels_==i])):\n",
    "                        \n",
    "                    \n",
    "\n",
    "            medium_hat=np.vstack(ghat_list_cores)\n",
    "            medium_hat=H.cat2complex(medium_hat)\n",
    "            medium_hat=medium_hat/np.linalg.norm(medium_hat, axis=1)[:,None]\n",
    "            torch_inners=np.abs(np.inner(medium_hat,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "            max_inners=np.max(torch_inners, axis=0)\n",
    "            num_recovered_list_CORES.append(len(max_inners[max_inners>T_hold]))\n",
    "\n",
    "            #print(max_inners[0],max(torch_inners[:,0]),medium_hat.shape)\n",
    "            #print(f'num recovered: {len(max_inners[max_inners>T_hold])}, inner avg: {max_inners.mean()}')\n",
    "\n",
    "            medium_hat_avgs=np.vstack(ghat_list_avgs)\n",
    "            medium_hat_avgs=H.cat2complex(medium_hat_avgs)\n",
    "            medium_hat_avgs=medium_hat_avgs/np.linalg.norm(medium_hat_avgs, axis=1)[:,None]\n",
    "            torch_inners_avgs=np.abs(np.inner(medium_hat_avgs,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j)\n",
    "            max_inners_avgs=np.max(torch_inners_avgs, axis=0)\n",
    "            #print(max_inners_avgs[0],max(torch_inners_avgs[:,0]))\n",
    "            #print(f'num recovered: {len(max_inners_avgs[max_inners_avgs>T_hold])}, inner avg: {max_inners_avgs.mean()}')\n",
    "            num_clusters.append(len(cluster_sizes))\n",
    "\n",
    "            num_recovered_list.append(len(max_inners_avgs[max_inners_avgs>T_hold]))\n",
    "            num_recovered_list_97.append(len(max_inners_avgs[max_inners_avgs>.97]))\n",
    "            num_really_bad.append(len(max_inners_avgs[max_inners_avgs<.95]))\n",
    "\n",
    "            eps_list.append(eps)\n",
    "            try:\n",
    "                Cluster_size_diff_list.append(sorted_cluster_sizes[-361]-sorted_cluster_sizes[-362])\n",
    "                next_largest_cluster.append(sorted_cluster_sizes[-362])\n",
    "            except:\n",
    "                Cluster_size_diff_list.append(0)\n",
    "                next_largest_cluster.append(sorted_cluster_sizes[0])\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            avg_cluster_size.append(np.mean(cluster_sizes))\n",
    "            try:\n",
    "                smallest_cluster_used.append(sorted_cluster_sizes[-361])\n",
    "            except:\n",
    "                smallest_cluster_used.append(sorted_cluster_sizes[0])\n",
    "\n",
    "            if len(max_inners_avgs[max_inners_avgs>T_hold])<=360:\n",
    "                ind_not_recovered_list.append(np.where(max_inners_avgs<T_hold)[0])\n",
    "\n",
    "    return num_clusters, num_recovered_list,num_recovered_list_97, num_really_bad, num_recovered_list_CORES, Cluster_size_diff_list,avg_cluster_size, smallest_cluster_used, next_largest_cluster, eps_list\n",
    "\n",
    "for min_centers in MIN_CENTERS_LIST:\n",
    "    num_clusters, num_recovered_list, num_recovered_list_97,num_really_bad,num_recovered_list_CORES,Cluster_size_diff_list,avg_cluster_size, smallest_cluster_used, next_largest_cluster, eps_list=DBSCAN_inners_fig(col_list, min_centers)\n",
    "    num_recovered_list=num_recovered_list_CORES\n",
    "    if len(eps_list)>0:\n",
    "        plt.figure()\n",
    "        plt.hlines(19**2, eps_list[0],eps_list[len(num_recovered_list)-1], colors='r', linestyles='dashed', label='True number of columns')  \n",
    "            #print('largest cluster, number clustered (dbscan), number of clusters=: ',max(cluster_sizes), sum(cluster_sizes),len(cluster_sizes)) #Num of )\n",
    "        #plt.plot(eps_linspace[:len(num_clusters)], num_clusters, label='DBSCAN number of clusters')\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)# Add subticks to the y-axis\n",
    "        plt.grid(axis='y', linestyle='dotted', which='both')  # Add subgridlines to the y-axis\n",
    "        plt.tight_layout()\n",
    "        R=-1\n",
    "        L=1\n",
    "        #plt.plot(eps_list[L:R], num_recovered_list[L:R], label='Number of columns recovered')\n",
    "        plt.plot(eps_list[L:R], num_recovered_list_97[L:R], label='Number of columns recovered with .97`')\n",
    "        plt.plot(eps_list[L:R], num_recovered_list[L:R], label='Number of columns recovered')\n",
    "        #plt.hlines(19**2, eps_list[L],eps_list[R], colors='r', linestyles='dashed', label='True number of columns')  \n",
    "            #print('largest cluster, number clustered (dbscan), number of clusters=: ',max(cluster_sizes), sum(cluster_sizes),len(cluster_sizes)) #Num of )\n",
    "        plt.plot(eps_list[L:R], num_clusters[L:R], label='Number of clusters')\n",
    "        plt.plot(eps_list[L:R], num_really_bad[L:R], label='Number of columns recovered with less than .95')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.figure()\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)# Add subticks to the y-axis\n",
    "        plt.grid(axis='y', linestyle='dotted', which='both')  # Add subgridlines to the y-axis\n",
    "        plt.tight_layout()\n",
    "        #plt.plot(eps_list[L:R], Cluster_size_diff_list[L:R], label='Cluster size difference')\n",
    "        plt.plot(eps_list[L:R], avg_cluster_size[L:R], label='Average cluster size')\n",
    "        plt.plot(eps_list[L:R], smallest_cluster_used[L:R], label='Smallest cluster used')\n",
    "        plt.plot(eps_list[L:R], next_largest_cluster[L:R], label='Next largest cluster')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "        #plt.savefig(f'/home/achristie/Codes_data/Experiment_data/rhosupport_stats/FL_paper/DBSCAN_inners_fig_{min_centers}.pdf')\n",
    "        #plt.close()#Takes 7 minouts to run\n",
    "index_of_shittiest=[]\n",
    "num_times_recovered=0\n",
    "indivual_recoveries=[0]*361\n",
    "total_num_cols=0\n",
    "for j in range(len(decoder_locats)):\n",
    "    col_list=get_cluster_data([decoder_locats[j]])\n",
    "    total_num_cols+=len(col_list)\n",
    "    num_times_recovered_list=[]\n",
    "    inn_list_max_list=[]\n",
    "    for i in range(361):\n",
    "        G_i=medium.T[i]\n",
    "        inn_list=abs(np.inner(G_i, col_list.conj()))\n",
    "        inn_list_max_list.append(np.max(inn_list))\n",
    "        #num_times_recovered=len(inn_list[inn_list>.99])\n",
    "        num_times_recovered_list.append(num_times_recovered)\n",
    "        num_times_recovered+=len(inn_list[inn_list>.99])\n",
    "        indivual_recoveries[i]+=len(inn_list[inn_list>.99])\n",
    "        \n",
    "\n",
    "\n",
    "Pop_mean=num_times_recovered/(total_num_cols)\n",
    "#indivual_recoveries_missed=np.array([50-i for i in indivual_recoveries])\n",
    "indivual_recoveries=np.array(indivual_recoveries)\n",
    "p_hat=Pop_mean\n",
    "expected_recoveries=np.array([Pop_mean*(total_num_cols/361)])\n",
    "\n",
    "\n",
    "rho_hat=np.zeros((2, 361))\n",
    "\n",
    "rho_hat[0, :]=indivual_recoveries\n",
    "rho=torch.tensor(rho)\n",
    "rho_hat=torch.tensor(rho_hat)\n",
    "#rho[22+20]=1\n",
    "#rho[400-22-1-20]=1\n",
    "#rho[400-37-1-20]=1\n",
    "#rho[210]=1\n",
    "\n",
    "H.plot_2_imgs(rho, rho_hat,ind=0, figsize=5, scaling=None, xpix=19, ypix=19, font_size=25, Single=True, Lclim=0, Hclim=75)\n",
    "print(f'min number of recoveries {min(indivual_recoveries)}, max number of recoveries {max(indivual_recoveries)}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

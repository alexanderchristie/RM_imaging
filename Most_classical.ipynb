{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Notebooks for small tests\n",
    "import os  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "plt.rcParams['axes.facecolor']='w'\n",
    "plt.rcParams['savefig.facecolor']='w'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "#imports and plotting function\n",
    "import argparse\n",
    "import mat73\n",
    "import logging\n",
    "import numpy as np\n",
    "#import torchvision\n",
    "from datetime import datetime\n",
    "import os\n",
    "import scipy.io\n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import Models as M\n",
    "import Helpers as H\n",
    "import time\n",
    "import copy\n",
    "from torch.func import vmap\n",
    "from functorch.experimental import replace_all_batch_norm_modules_\n",
    "%matplotlib inline\n",
    "\n",
    "encoder_out='sigmoid'\n",
    "cwd=os. getcwd()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "print(medium.shape)\n",
    "inners=medium.transpose().conjugate()@medium\n",
    "print(inners.shape)\n",
    "coherence=0\n",
    "for i in inners:\n",
    "    for j in i:\n",
    "        if abs(j)>coherence and j<.99:\n",
    "            coherence=abs(j)\n",
    "print('coherence of data: ', coherence)\n",
    "#b=np.load(data_path+'/train/b.npy')\n",
    "#rho=np.load(data_path+'/train/rho.npy')\n",
    "#print(np.allclose(medium@rho[0],b[0]))\n",
    "index_list=[]\n",
    "torch.__version__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starttime=time.time()\n",
    "target='raw'\n",
    "unlabeled_data=5000\n",
    "batchsize=128\n",
    "L1_weight_og=1e-1\n",
    "L1_weight=L1_weight_og\n",
    "L1_weight_final=L1_weight_og\n",
    "betas=(.9,.999)\n",
    "\n",
    "AMSGRAD=False\n",
    "THRESH_HOLD_VAL=.001\n",
    "layers=[2048*2,2048]#[1024,512] 3613, #[1024,256] 35 at 2500\n",
    "#[500,500,500,300,300,400,800]#[3000,1000,500,300,300,300,300,300,300,300,300,300,300,300]#[3000,1500,1000,500,500,500,300,300,300,400,400,800]#[1000,1000,1000,500,500,500,300,300,300,400,400,400,800]\n",
    "GELMA_layers=[2048*2,2048]#[128]*10#[500,500,500,300,300,400,800]\n",
    "CE=False\n",
    "KM_in=False\n",
    "INV_weight=0\n",
    "RESHUFFLE=False\n",
    "LR=1e-3\n",
    "LR_final=LR\n",
    "GELMA_OG=1e-1#1e-10\n",
    "GELMA=GELMA_OG\n",
    "GELMA_Final=GELMA_OG\n",
    "TIME_reverse=False\n",
    "Scheduling=False\n",
    "pixels='Gaussian_abs'\n",
    "E_list=0\n",
    "RESETS=0\n",
    "NORMALIZE=False\n",
    "G_0=True\n",
    "Epochs=3000\n",
    "index_list=[]\n",
    "GELMA_inc=0\n",
    "index_list=[]\n",
    "GELMA_inc=0\n",
    "tanh=nn.Tanh()\n",
    "softmax=nn.Softmax(dim=1)\n",
    "def soft_threh(x, th):\n",
    "    #print(x.shape)\n",
    "    x=x.squeeze()\n",
    "    real, imag=torch.split(x, int(x.shape[-1]/2), dim=-1)\n",
    "    modulus=torch.sqrt(real**2+imag**2)\n",
    "    modulus=relu(modulus-th)\n",
    "\n",
    "    #modulus=softmax(modulus)\n",
    "    #max_mod, _=torch.max(modulus, dim=-1, keepdim=True)\n",
    "    #modulus=modulus/max_mod\n",
    "\n",
    "    \n",
    "    #topk, indices=torch.topk(modulus, 5, dim=-1)\n",
    "    #modulus=torch.zeros_like(modulus)\n",
    "    #modulus=modulus.scatter(1,indices, topk)\n",
    "    #modulus=modulus.squeeze()\n",
    "     \n",
    "    theta=torch.atan2(imag, real).squeeze()\n",
    "    real_out=modulus*torch.cos(theta)\n",
    "    imag_out=modulus*torch.sin(theta) \n",
    "    x=torch.cat((real_out,imag_out),-1) #relu(torch.abs(x)-th)\n",
    "    #x=F.normalize(x,dim=-1)\n",
    "    \n",
    "    return x#torch.cat((real_out,imag_out),-1) #relu(torch.abs(x)-th)\n",
    "#data_path=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "data_path=os.path.join(cwd,'Data/FoldyLox_all_seeds/FoldyLox_seed0')\n",
    "#data_path=os.path.join(cwd,'Data/PNAS-highcoh_regime_all_seeds/PNAS-highcoh_regime_seed0')\n",
    "\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "print(medium.shape)\n",
    "inners=medium.transpose().conjugate()@medium\n",
    "print(inners.shape)\n",
    "coherence=0\n",
    "for i in inners:\n",
    "    for j in i:\n",
    "        if abs(j)>coherence and j<.99:\n",
    "            coherence=abs(j)\n",
    "print('coherence of data: ', coherence)\n",
    "\n",
    "\n",
    "Track_run=False\n",
    "#Enter your wanbd key below and uncomment wanbd code to track run on wandb\n",
    "if Track_run:\n",
    "    key='89a70fbc572a495206df640bd6c9cbf2a4a0dcaa' #enter your own key here\n",
    "    wandb.login(key=key) \n",
    "    wandb.init(project='rtt unlabeled', name=f'Classical Gelma, longer')\n",
    "\n",
    "    wandb.config['L1 final weight']=L1_weight_final\n",
    "    wandb.config['unlabeled_data']=unlabeled_data \n",
    "    wandb.config['L1_weight']=L1_weight\n",
    "    wandb.config['layers']=layers\n",
    "    wandb.config['GELMA']=GELMA\n",
    "    wandb.config['G_0']=G_0\n",
    "    wandb.config['GELMA_layers']=GELMA_layers   \n",
    "    wandb.config['inv_weight']=INV_weight\n",
    "    wandb.config['CE']=CE\n",
    "wand_dict={}\n",
    "#GELMA_layers.append(500)\n",
    "#layers.append(500)\n",
    "training_data=H.data_rho_loaded(data_path+'/train',unlabeled_data/80000,pixels=pixels, normalize=NORMALIZE)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax=nn.Softmax(dim=0)\n",
    "#training_data.b=training_data.b.to(device)\n",
    "#training_data.rho=training_data.rho.to(device)\n",
    "#rh_t, _=torch.split(rho, 400, dim=-1)\n",
    "\n",
    "\n",
    "val_data=H.data_rho_loaded(data_path+'/val', 3000/80000, pixels=pixels, normalize=NORMALIZE)\n",
    "b_val=val_data.b.to(device)\n",
    "rho_val=val_data.rho.to(device)\n",
    "indim=int(training_data.b[0].shape[0]/2)\n",
    "outdim=(training_data.rho[0].shape[0]/4)\n",
    "enc_dim=training_data.b[0].shape[0]/2\n",
    "if KM_in:\n",
    "    enc_dim=enc_dim+outdim*2\n",
    "if pixels=='Gaussian_abs':\n",
    "    enc_out_dim=outdim*2\n",
    "    dec_in=int(training_data.rho[0].shape[0])\n",
    "\n",
    "else:\n",
    "    enc_out_dim=outdim\n",
    "    dec_in=int(training_data.rho[0].shape[0]/2)\n",
    "decoder_hats=[]\n",
    "encoder=M.fc_net_extra(enc_dim, layers,enc_out_dim, net_type='fc',linear_type='real', activation='leaky', bias=True, out_scaling=None,dropout=.5)\n",
    "#encoder=M.channeled_lin_layers_avg(enc_dim, layers, 10,int(outdim*2), dropout=.5)\n",
    "\n",
    "print(H.count_parameters(encoder))\n",
    "#decoder=nn.Linear(int(training_data.rho[0].shape[0]/2), int(training_data.b[0].shape[0]), bias=False)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "#decoder=M.norm_linear(int(training_data.rho[0].shape[0]), int(training_data.b[0].shape[0]), normalize=False)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "decoder=M.norm_linear_complex(int(training_data.rho[0].shape[0]/2), int(training_data.b[0].shape[0]), normalize=NORMALIZE)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "\n",
    "#decoder=M.fc_net_extra(outdim, layers[::-1], indim, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None,dropout=.5)\n",
    "if G_0:\n",
    "    #G_0_w=(np.array(mat73.loadmat(data_path+'/G_0.mat')['A0']))\n",
    "    \n",
    "    G_0_w_np=(np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt']))\n",
    "    G_0_w_np=G_0_w_np+np.random.normal(0, .01, G_0_w_np.shape)+1j*np.random.normal(0, .01, G_0_w_np.shape)\n",
    "    \n",
    "    #G_0_w_np=torch.tensor(G_0_w_np).float()\n",
    "\n",
    "\n",
    "    #G_0_w=torch.cat((torch.tensor(G_0_w_np.real), torch.tensor(G_0_w_np.imag)), dim=0)\n",
    "    #G_0_w_2=torch.cat((torch.tensor(G_0_w_np.real), -torch.tensor(G_0_w_np.imag)), dim=0)\n",
    "    #G_0_w=torch.cat((G_0_w, G_0_w_2), dim=1)\n",
    "    #G_0_w=G_0_w.float()\n",
    "    decoder.weight_real.data=nn.parameter.Parameter(torch.tensor(G_0_w_np.real).float().clone().detach().requires_grad_(True))\n",
    "    decoder.weight_imag.data=nn.parameter.Parameter(torch.tensor(G_0_w_np.imag).float().clone().detach().requires_grad_(True))\n",
    "\n",
    "    if pixels=='Gaussian_abs':\n",
    "        Complex_eye=torch.cat((torch.eye(int(outdim*2)), torch.zeros((int(outdim*2),int(outdim*2)))), dim=1)\n",
    "    else:\n",
    "        Complex_eye=torch.eye(int(outdim*2)).unsqueeze(1)\n",
    "    #Complex_eye=torch.eye(int(outdim*4))\n",
    "    Complex_eye=torch.cat((torch.eye(int(outdim*2)), torch.zeros((int(outdim*2),int(outdim*2)))), dim=1)\n",
    "    \n",
    "#    print(Complex_eye.shape,    G_0_w.shape)\n",
    "    medium_hat=decoder(Complex_eye).squeeze()\n",
    "    medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "print(H.count_parameters(encoder))\n",
    "if Track_run:\n",
    "    wandb.config['encoder params']=H.count_parameters(encoder)\n",
    "print('Data shapes:', training_data.b[0].shape[0]/2, training_data.rho[0].shape[0]/4)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "relu=nn.ReLU()\n",
    "leaky_relu=nn.LeakyReLU(THRESH_HOLD_VAL)\n",
    "optimizer = torch.optim.AdamW(encoder.parameters(), lr=LR,betas=betas, amsgrad=AMSGRAD,  weight_decay=1e-5)\n",
    "optimizer_decod = torch.optim.AdamW(decoder.parameters(), lr=LR, betas=betas, amsgrad=AMSGRAD, weight_decay=1e-5)\n",
    "True_l1_avg=0\n",
    "\n",
    "tanh=nn.Tanh()\n",
    "softmax=nn.Softmax(dim=1)\n",
    "bce_loss_fn=nn.BCELoss() \n",
    "Threshold=nn.Threshold(THRESH_HOLD_VAL, 0)  \n",
    "L2_loss_fn=nn.MSELoss()\n",
    "CE_loss_fn=nn.CrossEntropyLoss()\n",
    "#L2_loss_fn=lambda x,y: torch.sqrt(nn.MSELoss()(x,y))\n",
    "L1_loss_fn=nn.L1Loss()\n",
    "if pixels=='Gaussian_abs':\n",
    "    Complex_eye=torch.cat((torch.eye(int(outdim*2)), torch.zeros((int(outdim*2),int(outdim*2)))), dim=1).to(device)\n",
    "else:\n",
    "    Complex_eye=torch.eye(int(outdim*2)).unsqueeze(1).to(device)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "Complex_eye=torch.eye(int(outdim*4)).unsqueeze(1).to(device)\n",
    "\n",
    "if GELMA>0:\n",
    "    GELMA_net=M.fc_net_extra(training_data.b[0].shape[0]/2, GELMA_layers, training_data.b[0].shape[0]/2, net_type='fc',linear_type='real', activation='leaky', bias=True, out_scaling=None, dropout=.5)\n",
    "    optimizer_GELMA = torch.optim.AdamW(GELMA_net.parameters(), lr=LR, maximize=True, betas=betas, amsgrad=AMSGRAD, weight_decay=1e-5)\n",
    "    lr_scheduler_GELMA=torch.optim.lr_scheduler.ExponentialLR(optimizer_GELMA, gamma=(LR_final/LR)**(1/Epochs), last_epoch=-1)\n",
    "    GELMA_net.to(device)\n",
    "    #GELMA_net=nn.DataParallel(GELMA_net)\n",
    "    GELMA_net.train()\n",
    "    #GELMA_net.set_params_to_zero()\n",
    "    #encoder.set_params_to_zero()\n",
    "def f_col(batch):\n",
    "    b=torch.stack([item[0] for item in batch])\n",
    "    rho=torch.stack([item[1] for item in batch])\n",
    "    num_targets=torch.stack([item[2] for item in batch])\n",
    "    return b.to(device), rho.to(device), num_targets\n",
    "\n",
    "\n",
    "def rescale_rho(z):\n",
    "    minimum, _=torch.min(z, -1, keepdim=True)\n",
    "    maxium, _=torch.max(z, -1,   keepdim=True)\n",
    "    z=(z-minimum)/(maxium-minimum)\n",
    "    return z\n",
    "lr_scheduler_enc=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=(LR_final/LR)**(1/Epochs), last_epoch=-1)\n",
    "lr_scheduler_decod=torch.optim.lr_scheduler.ExponentialLR(optimizer_decod, gamma=(LR_final/LR)**(1/Epochs), last_epoch=-1)\n",
    "trainloader=DataLoader(training_data,batch_size=batchsize,shuffle=True,num_workers=0)\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    if RESETS>0 and epoch%RESETS==0:\n",
    "        encoder=M.fc_net_extra(enc_dim, layers,outdim, net_type='fc',linear_type='real', activation='leaky', bias=True, out_scaling=None,dropout=.5)\n",
    "        encoder.to(device)\n",
    "        optimizer = torch.optim.AdamW(encoder.parameters(), lr=LR)\n",
    "        optimizer_decod = torch.optim.AdamW(decoder.parameters(), lr=LR)\n",
    "    train_lossavg=0\n",
    "    train_lossavg=0\n",
    "    L1_loss_avg=0\n",
    "    coupled_grad=0\n",
    "    project_grad=0\n",
    "    INV_loss_avg=0\n",
    "    CEloss_avg=0\n",
    "\n",
    "\n",
    "    inner_loss_term_avg=0\n",
    "    True_l1_avg=0\n",
    "    for batch, (b, rho, num_targets) in enumerate(trainloader):\n",
    "        if True:\n",
    "            b=b.to(device)\n",
    "            rho=rho.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_decod.zero_grad()\n",
    "            rho_hat=encoder(b)\n",
    "            rho_hat=soft_threh(rho_hat, THRESH_HOLD_VAL)\n",
    "            #rho_hat=relu(abs(rho_hat)-.005)*torch.sign(rho_hat)            \n",
    "            #rho_hat=softmax(rho_hat)\n",
    "            #max_rho, _=torch.max(abs(rho_hat), dim=-1, keepdim=True)\n",
    "            #rho_hat=abs(rho_hat)/max_rho\n",
    "            #rho_hat=sigmoid(rho_hat)\n",
    "            b_hat=decoder(rho_hat)\n",
    "            if L1_weight>0:\n",
    "                #L1_rhoreal, L1_rhoimag=torch.split(rho_hat, int(rho_hat.shape[-1]/2), dim=-1)\n",
    "                #L1_rho=(L1_rhoreal**2+L1_rhoimag**2)**.5\n",
    "                True_l1=L1_loss_fn(rho,rho*0)\n",
    "                True_l1_avg+=True_l1.item()/len(trainloader)\n",
    "                \n",
    "                L1_loss=L1_weight*L1_loss_fn(rho_hat, rho_hat*0)\n",
    "                L1_loss.backward(retain_graph=True)  \n",
    "                L1_loss_avg+=L1_loss.item()/L1_weight/len(trainloader)\n",
    "            else:\n",
    "                L1_loss=-1\n",
    "            L2_loss=L2_loss_fn(b_hat.squeeze(), b.squeeze())\n",
    "#            L2_loss=L2_loss_fn(F.normalize(b_hat.squeeze(), dim=-1), F.normalize(b.squeeze()))\n",
    "            L2_loss.backward(retain_graph=True)\n",
    "            train_lossavg+=L2_loss.item()/len(trainloader)\n",
    "\n",
    "            #if GELMA==0:\n",
    "            \n",
    "            if GELMA==0:\n",
    "                optimizer.step()\n",
    "                optimizer_decod.step()\n",
    "                inner_loss_term_avg=-1\n",
    "\n",
    "\n",
    "            elif GELMA>0 and epoch>0:\n",
    "                #optimizer_GELMA.zero_grad()\n",
    "                #optimizer.zero_grad()\n",
    "                #optimizer_decod.zero_grad()\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                GELMA_out=GELMA_net(b).squeeze()\n",
    "                GELMA_out=soft_threh(GELMA_out, THRESH_HOLD_VAL)\n",
    "                #GELMA_out=F.normalize(GELMA_out, dim=-1).squeeze()\n",
    "                b_hat=b_hat.squeeze()\n",
    "                \n",
    "                GELMA_inners_coupled=(torch.inner((-b_hat).squeeze(),GELMA_out.squeeze()).diagonal(dim1=-2, dim2=-1))\n",
    "                GELMA_inners_coupled=sum(GELMA_inners_coupled)/(torch.numel(GELMA_inners_coupled))/b.shape[-1]\n",
    "                GELMA_inners_project=(torch.inner((b).squeeze(),GELMA_out.squeeze()).diagonal(dim1=-2, dim2=-1))\n",
    "                GELMA_inners_project=sum(GELMA_inners_project)/(torch.numel(GELMA_inners_project))/b.shape[-1]\n",
    "                GELMA_inners=GELMA*(GELMA_inners_project+GELMA_inners_coupled)\n",
    "            \n",
    "\n",
    "                GELMA_inners.backward(retain_graph=True)          \n",
    "                inner_loss_term_avg+=GELMA_inners.item()/len(trainloader)/GELMA\n",
    "                optimizer_decod.step()\n",
    "            \n",
    "\n",
    "\n",
    "                optimizer_GELMA.step()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                optimizer_GELMA.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                optimizer_decod.zero_grad()\n",
    "\n",
    "          \n",
    "\n",
    "    if L1_weight>0:\n",
    "        L1_weight=min(L1_weight*(L1_weight_final/L1_weight_og)**(1/(Epochs/2)), L1_weight_final)\n",
    "    if GELMA>0:\n",
    "        GELMA=min(GELMA*((GELMA_Final/GELMA_OG)**(1/(Epochs/2))), GELMA_Final)\n",
    "    if Scheduling and epoch>Epochs/2:\n",
    "        lr_scheduler_enc.step()\n",
    "        lr_scheduler_decod.step()\n",
    "        if GELMA>0:\n",
    "            lr_scheduler_GELMA.step()\n",
    "    val_lossavg=0\n",
    "    if epoch%50==49:\n",
    "        if pixels=='Gaussian_abs':\n",
    "            rho_hat=torch.abs(H.cat2complex(rho_hat.squeeze()))\n",
    "        H.plot_2_imgs(rho, rho_hat,ind=5, figsize=5, scaling=None, xpix=19, ypix=19, font_size=25)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    #if GELMA_inc>0 and GELMA<GELMA_Final:\n",
    "    #    GELMA=GELMA*GELMA_inc\n",
    "    #    if GELMA>GELMA_Final:\n",
    "    #        GELMA=GELMA_Final\n",
    "    #        print('GELMA max reached')\n",
    "\n",
    "    if INV_weight>0 and INV_weight<1e-1:\n",
    "        INV_weight=INV_weight*(1.03)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_decod.zero_grad()\n",
    "\n",
    "    font_size = 50\n",
    "    if KM_in:\n",
    "        medium_hat=decoder(Complex_eye).squeeze()\n",
    "        medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "        Mhat_real, Mhat_imag=torch.split(medium_hat, int(medium_hat.shape[1]/2), dim=-1)\n",
    "        Mhat_imag=-Mhat_imag\n",
    "        b_real, b_imag=torch.split(b_val.squeeze(), int(b_val.shape[-1]/2), dim=-1)\n",
    "        km_b_real=Mhat_real.squeeze()@b_real.squeeze().T-Mhat_imag.squeeze()@b_imag.squeeze().T\n",
    "        km_b_imag=Mhat_real.squeeze()@b_imag.squeeze().T+Mhat_imag.squeeze()@b_real.squeeze().T\n",
    "\n",
    "        km_b=torch.cat((km_b_real.T, km_b_imag.T), dim=-1)\n",
    "        \n",
    "        In_put=torch.cat((b_val, km_b), dim=-1)\n",
    "\n",
    "        rho_hat=encoder(In_put)\n",
    "    else:\n",
    "        rho_hat=encoder(b_val)\n",
    "        rho_hat=soft_threh(rho_hat, THRESH_HOLD_VAL)\n",
    "\n",
    "        #rhos_hat=relu(abs(rho_hat)-.005)*torch.sign(rho_hat)            \n",
    "\n",
    "    #rho_hat=sigmoid(rho_hat)\n",
    "    #rho_hat=relu(rho_hat)\n",
    "    #rho_hat=leaky_relu(rho_hat)\n",
    "    #rho_hat=softmax(rho_hat)\n",
    "\n",
    "    #max_rho, _=torch.max(rho_hat, dim=-1, keepdim=True)\n",
    "    #rho_hat=rho_hat/max_rho\n",
    "    #rho_hat=rescale_rho(rho_hat)\n",
    "    #rho_hat=Threshold(abs(rho_hat))\n",
    "    #max_rho, _=torch.max(rho_hat, dim=-1, keepdim=True) f\n",
    "    #rho_hat=rho_hat/max_rho\n",
    "    #rho_hat=tanh(rho_hat)\n",
    "\n",
    "    b_hat=decoder(rho_hat) \n",
    "  #  L2_loss=L2_loss_fn(F.normalize(b_hat.squeeze(), dim=-1), F.normalize(b_val.squeeze()))\n",
    "\n",
    "    L2_loss=L2_loss_fn(b_hat.squeeze(), b_val.squeeze())\n",
    "    val_lossavg+=L2_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if epoch%50==49:\n",
    "        if pixels=='Gaussian_abs':\n",
    "            rho_hat=torch.abs(H.cat2complex(rho_hat.squeeze()))\n",
    "        H.plot_2_imgs(rho_val, rho_hat,ind=5, figsize=5, scaling=None, xpix=19, ypix=19, font_size=25)\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        sum_max_inner_original=0\n",
    "        medium_hat=decoder(Complex_eye).squeeze()\n",
    "        medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "        medium_hat=medium_hat.cpu().detach().numpy()\n",
    "        medium_hat=H.cat2complex(medium_hat)                    \n",
    "        torch_inners=np.abs(np.inner(medium_hat,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "                #original way of computing inners\n",
    "                # mean over true max over hats\n",
    "        index_list=[]\n",
    "        for i in range(int(outdim*2)):\n",
    "            if max(torch_inners[:, i])>.99 and i not in index_list:\n",
    "                index_list.append(i)\n",
    "            sum_max_inner_original=sum_max_inner_original+max(torch_inners[:, i])\n",
    "        max_avg_inners_original=sum_max_inner_original/(int(outdim*2))\n",
    "\n",
    "        \n",
    "        #lr_scheduler_enc.step()\n",
    "        #lr_scheduler_decod.step()\n",
    "        \n",
    "        \n",
    "        print(f'epoch: {epoch}, train loss: {train_lossavg}, L1 loss: {L1_loss_avg}, val loss: {val_lossavg}, max avg inners: {max_avg_inners_original}, GELMA loss: {inner_loss_term_avg}, num indices: {len(index_list)}, true L1 loss: {True_l1_avg}')\n",
    "        #print('True L1 loss:', True_l1_avg)\n",
    "        #print(f'coupled grad: {coupled_grad}, project grad: {project_grad}')\n",
    "        wand_dict['train loss']=train_lossavg\n",
    "        wand_dict['val loss']=val_lossavg\n",
    "        if L1_loss>0:\n",
    "            wand_dict['L1 loss']=L1_loss_avg\n",
    "        if GELMA>0:\n",
    "            wand_dict['GELMA loss']=inner_loss_term_avg\n",
    "        if CE>0:\n",
    "            wand_dict['CE loss']=CEloss_avg\n",
    "        if INV_weight>0:\n",
    "            wand_dict['INV loss']=INV_loss_avg\n",
    "\n",
    "    \n",
    "        wand_dict['max avg inners']=max_avg_inners_original\n",
    "        wand_dict['num indices']=len(index_list)\n",
    "    if Track_run:\n",
    "        wandb.log(wand_dict)\n",
    "\n",
    "print(time.time()-starttime)\n",
    "if Track_run:\n",
    "    wandb.finish()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_inners.shape,Complex_eye.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0.shape[0]/2, rho0.shape[0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium.shape, medium_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_max_inner_original=0\n",
    "medium_hat=decoder(Complex_eye.squeeze()).squeeze()\n",
    "medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "medium_hat=medium_hat.cpu().detach().numpy()\n",
    "medium_hat=H.cat2complex(medium_hat)                    \n",
    "torch_inners=np.abs(np.inner(medium_hat,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "        \n",
    "index_list=[]\n",
    "for i in range(int(outdim*2)):\n",
    "    if (np.max(torch_inners[:, i]))>.99 and i not in index_list:\n",
    "        index_list.append(i)\n",
    "    sum_max_inner_original=sum_max_inner_original+np.max(torch_inners[:, i])\n",
    "max_avg_inners_original=sum_max_inner_original/(int(outdim*2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workstation1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

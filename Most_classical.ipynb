{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1450, 400)\n",
      "(400, 400)\n",
      "coherence of data:  0.7119395644427655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Notebooks for small tests\n",
    "import os  \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "plt.rcParams['axes.facecolor']='w'\n",
    "plt.rcParams['savefig.facecolor']='w'\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "#imports and plotting function\n",
    "import argparse\n",
    "import mat73\n",
    "import logging\n",
    "import numpy as np\n",
    "#import torchvision\n",
    "from datetime import datetime\n",
    "import os\n",
    "import scipy.io\n",
    "from torch.utils.data import DataLoader\n",
    "import io\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import sys\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import Models as M\n",
    "import Helpers as H\n",
    "import time\n",
    "import copy\n",
    "from torch.func import vmap\n",
    "from functorch.experimental import replace_all_batch_norm_modules_\n",
    "%matplotlib inline\n",
    "\n",
    "encoder_out='sigmoid'\n",
    "cwd=os. getcwd()\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "print(medium.shape)\n",
    "inners=medium.transpose().conjugate()@medium\n",
    "print(inners.shape)\n",
    "coherence=0\n",
    "for i in inners:\n",
    "    for j in i:\n",
    "        if abs(j)>coherence and j<.99:\n",
    "            coherence=abs(j)\n",
    "print('coherence of data: ', coherence)\n",
    "#b=np.load(data_path+'/train/b.npy')\n",
    "#rho=np.load(data_path+'/train/rho.npy')\n",
    "#print(np.allclose(medium@rho[0],b[0]))\n",
    "index_list=[]\n",
    "torch.__version__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806, 361)\n",
      "(361, 361)\n",
      "coherence of data:  0.763143828697641\n",
      "Medium: (806, 361), Rho: (5000, 361), B: (5000, 806)\n",
      "Medium: (806, 361), Rho: (3000, 361), B: (3000, 806)\n",
      "49756278\n",
      "49756278\n",
      "Data shapes: 806.0 180.5\n",
      "epoch: 0, train loss: 0.8964261040091513, L1 loss: 0.6454518288373947, val loss: 0.027463069185614586, max avg inners: 0.9277025805629314, GELMA loss: 0, num indices: 0, true L1 loss: 0.007137140748091042\n",
      "epoch: 1, train loss: 1.073010386549868, L1 loss: 0.21796279679983854, val loss: 0.00536633376032114, max avg inners: 0.8942557726964373, GELMA loss: -0.0023154728301051364, num indices: 0, true L1 loss: 0.00713496347889304\n",
      "epoch: 2, train loss: 0.0033286808175034817, L1 loss: 0.04873670596862212, val loss: 0.0023764469660818577, max avg inners: 0.8929960810803328, GELMA loss: -0.0030222280620364472, num indices: 0, true L1 loss: 0.007136281847488133\n",
      "epoch: 3, train loss: 0.00201719676842913, L1 loss: 0.023766559635987505, val loss: 0.002211733255535364, max avg inners: 0.8929774613315427, GELMA loss: -0.0013352278638194548, num indices: 0, true L1 loss: 0.007131483359262347\n",
      "epoch: 4, train loss: 0.0017579958861460906, L1 loss: 0.018404964939691126, val loss: 0.001810967456549406, max avg inners: 0.8929995284351879, GELMA loss: 0.0006583653407119527, num indices: 0, true L1 loss: 0.007140608655754478\n",
      "epoch: 5, train loss: 0.0018385843315627426, L1 loss: 0.016768835252150893, val loss: 0.001737908460199833, max avg inners: 0.8931354229348565, GELMA loss: 0.0015082021200214513, num indices: 0, true L1 loss: 0.007140772207640111\n",
      "epoch: 6, train loss: 0.001505348976934329, L1 loss: 0.016103716363431886, val loss: 0.0015721004456281662, max avg inners: 0.8931737332523619, GELMA loss: 0.002236956124761491, num indices: 0, true L1 loss: 0.007137472345493736\n",
      "epoch: 7, train loss: 0.0013508855772670358, L1 loss: 0.01459411473479122, val loss: 0.0015241357032209635, max avg inners: 0.8932153790186467, GELMA loss: 0.0025690423499327153, num indices: 0, true L1 loss: 0.00713682264322415\n",
      "epoch: 8, train loss: 0.0012766562198521568, L1 loss: 0.013963453442556784, val loss: 0.0015093607362359762, max avg inners: 0.893266329983618, GELMA loss: 0.003116660605883226, num indices: 0, true L1 loss: 0.0071399355540052055\n",
      "epoch: 9, train loss: 0.0012967437854968008, L1 loss: 0.014523040794301778, val loss: 0.0014383784728124738, max avg inners: 0.8933108805109323, GELMA loss: 0.004378620309580583, num indices: 0, true L1 loss: 0.007137321389745921\n",
      "epoch: 10, train loss: 0.0012112829077523205, L1 loss: 0.013087733939755708, val loss: 0.001308405539020896, max avg inners: 0.8933663459741518, GELMA loss: 0.00613977869215887, num indices: 0, true L1 loss: 0.007137766445521264\n",
      "epoch: 11, train loss: 0.0011682961805490775, L1 loss: 0.012642197863897309, val loss: 0.001389286364428699, max avg inners: 0.8934278542137983, GELMA loss: 0.007742174493614584, num indices: 0, true L1 loss: 0.0071312739164568475\n",
      "epoch: 12, train loss: 0.0011445068928878752, L1 loss: 0.012802595214452595, val loss: 0.0013589255977422, max avg inners: 0.8934966339486474, GELMA loss: 0.009062550336238928, num indices: 0, true L1 loss: 0.007135819201357663\n",
      "epoch: 13, train loss: 0.0010865178453968837, L1 loss: 0.012690591102000326, val loss: 0.0012348729651421309, max avg inners: 0.8935666876937015, GELMA loss: 0.010107887006597593, num indices: 0, true L1 loss: 0.007134954596403986\n",
      "epoch: 14, train loss: 0.0010278905334416778, L1 loss: 0.012306236516451463, val loss: 0.0011927192099392414, max avg inners: 0.8936451697568765, GELMA loss: 0.010930967837339267, num indices: 0, true L1 loss: 0.007141227729152889\n",
      "epoch: 15, train loss: 0.0009946624501026235, L1 loss: 0.012659123865887523, val loss: 0.0011669860687106848, max avg inners: 0.8937282036926999, GELMA loss: 0.011517960228957236, num indices: 0, true L1 loss: 0.007136756449472159\n",
      "epoch: 16, train loss: 0.0009655179819674231, L1 loss: 0.013089447224047035, val loss: 0.0011185145704075694, max avg inners: 0.8938156820403035, GELMA loss: 0.011899478820851073, num indices: 0, true L1 loss: 0.007139072788413615\n",
      "epoch: 17, train loss: 0.0009261729326681236, L1 loss: 0.012702209933195263, val loss: 0.0011248377850279212, max avg inners: 0.8939116257756938, GELMA loss: 0.012380145431961864, num indices: 0, true L1 loss: 0.007138408871833234\n",
      "epoch: 18, train loss: 0.0009199364081723617, L1 loss: 0.012996237230254337, val loss: 0.0011362554505467415, max avg inners: 0.8940134862274004, GELMA loss: 0.012852892454247922, num indices: 0, true L1 loss: 0.0071361660026013855\n",
      "epoch: 19, train loss: 0.0008907460316549985, L1 loss: 0.013144275086233392, val loss: 0.0011426355922594666, max avg inners: 0.8941215751843546, GELMA loss: 0.01311325398273766, num indices: 0, true L1 loss: 0.0071374880848452445\n",
      "epoch: 20, train loss: 0.0008936133235692977, L1 loss: 0.013401334668742493, val loss: 0.0010468399850651622, max avg inners: 0.8942338430147198, GELMA loss: 0.013011549599468708, num indices: 0, true L1 loss: 0.00713451700285077\n",
      "epoch: 21, train loss: 0.0008419680016231723, L1 loss: 0.01348982730996795, val loss: 0.0010919244959950447, max avg inners: 0.8943552870668408, GELMA loss: 0.013265344896353781, num indices: 0, true L1 loss: 0.007138257031328973\n",
      "epoch: 22, train loss: 0.0008359321567695585, L1 loss: 0.013767237833235413, val loss: 0.0010260972194373608, max avg inners: 0.8944840277167402, GELMA loss: 0.013447303848806769, num indices: 0, true L1 loss: 0.007141406403388827\n",
      "epoch: 23, train loss: 0.0008312586098327301, L1 loss: 0.013935089024016634, val loss: 0.0011767708929255605, max avg inners: 0.8946182995299246, GELMA loss: 0.013830111653078347, num indices: 0, true L1 loss: 0.007140632579103113\n",
      "epoch: 24, train loss: 0.0008563815237721428, L1 loss: 0.0143673854181543, val loss: 0.0010630732867866755, max avg inners: 0.8947504120564237, GELMA loss: 0.014235980284865946, num indices: 0, true L1 loss: 0.0071378714405000215\n",
      "epoch: 25, train loss: 0.0007798972903401592, L1 loss: 0.014636811596574262, val loss: 0.000929820176679641, max avg inners: 0.8948975773562359, GELMA loss: 0.014069774391828105, num indices: 0, true L1 loss: 0.007138443016447126\n",
      "epoch: 26, train loss: 0.0007395488733891397, L1 loss: 0.01436130455113016, val loss: 0.0008598306449130177, max avg inners: 0.8950605000989547, GELMA loss: 0.014296695298980922, num indices: 0, true L1 loss: 0.007134609366767106\n",
      "epoch: 27, train loss: 0.0007109750251402146, L1 loss: 0.014169387985020876, val loss: 0.0008899737149477005, max avg inners: 0.8952322188301735, GELMA loss: 0.014306561643024907, num indices: 0, true L1 loss: 0.007141969876829534\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 247\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m L1_weight\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m#L1_rhoreal, L1_rhoimag=torch.split(rho_hat, int(rho_hat.shape[-1]/2), dim=-1)\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m#L1_rho=(L1_rhoreal**2+L1_rhoimag**2)**.5\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     True_l1\u001b[38;5;241m=\u001b[39mL1_loss_fn(rho,rho\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 247\u001b[0m     True_l1_avg\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43mTrue_l1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(trainloader)\n\u001b[1;32m    249\u001b[0m     L1_loss\u001b[38;5;241m=\u001b[39mL1_weight\u001b[38;5;241m*\u001b[39mL1_loss_fn(rho_hat, rho_hat\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    250\u001b[0m     L1_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "starttime=time.time()\n",
    "target='raw'\n",
    "unlabeled_data=5000\n",
    "batchsize=128\n",
    "L1_weight_og=1e-1\n",
    "L1_weight=L1_weight_og\n",
    "L1_weight_final=L1_weight_og\n",
    "betas=(.9,.999)\n",
    "\n",
    "AMSGRAD=False\n",
    "THRESH_HOLD_VAL=.001\n",
    "layers=[2048*2,2048]#[1024,512] 3613, #[1024,256] 35 at 2500\n",
    "#[500,500,500,300,300,400,800]#[3000,1000,500,300,300,300,300,300,300,300,300,300,300,300]#[3000,1500,1000,500,500,500,300,300,300,400,400,800]#[1000,1000,1000,500,500,500,300,300,300,400,400,400,800]\n",
    "GELMA_layers=[2048*2,2048]#[128]*10#[500,500,500,300,300,400,800]\n",
    "CE=False\n",
    "KM_in=False\n",
    "INV_weight=0\n",
    "RESHUFFLE=False\n",
    "LR=1e-3\n",
    "LR_final=LR\n",
    "GELMA_OG=1e-1#1e-10\n",
    "GELMA=GELMA_OG\n",
    "GELMA_Final=GELMA_OG\n",
    "TIME_reverse=False\n",
    "Scheduling=False\n",
    "pixels='Gaussian_abs'\n",
    "E_list=0\n",
    "RESETS=0\n",
    "NORMALIZE=False\n",
    "G_0=True\n",
    "Epochs=3000\n",
    "index_list=[]\n",
    "GELMA_inc=0\n",
    "index_list=[]\n",
    "GELMA_inc=0\n",
    "tanh=nn.Tanh()\n",
    "softmax=nn.Softmax(dim=1)\n",
    "def soft_threh(x, th):\n",
    "    #print(x.shape)\n",
    "    x=x.squeeze()\n",
    "    real, imag=torch.split(x, int(x.shape[-1]/2), dim=-1)\n",
    "    modulus=torch.sqrt(real**2+imag**2)\n",
    "    modulus=relu(modulus-th)\n",
    "\n",
    "    #modulus=softmax(modulus)\n",
    "    #max_mod, _=torch.max(modulus, dim=-1, keepdim=True)\n",
    "    #modulus=modulus/max_mod\n",
    "\n",
    "    \n",
    "    #topk, indices=torch.topk(modulus, 5, dim=-1)\n",
    "    #modulus=torch.zeros_like(modulus)\n",
    "    #modulus=modulus.scatter(1,indices, topk)\n",
    "    #modulus=modulus.squeeze()\n",
    "     \n",
    "    theta=torch.atan2(imag, real).squeeze()\n",
    "    real_out=modulus*torch.cos(theta)\n",
    "    imag_out=modulus*torch.sin(theta) \n",
    "    x=torch.cat((real_out,imag_out),-1) #relu(torch.abs(x)-th)\n",
    "    #x=F.normalize(x,dim=-1)\n",
    "    \n",
    "    return x#torch.cat((real_out,imag_out),-1) #relu(torch.abs(x)-th)\n",
    "#data_path=os.path.join(cwd,'Data/PNAS-regime_all_seeds/PNAS-regime_seed0')\n",
    "data_path=os.path.join(cwd,'Data/FoldyLox_all_seeds/FoldyLox_seed0')\n",
    "#data_path=os.path.join(cwd,'Data/PNAS-highcoh_regime_all_seeds/PNAS-highcoh_regime_seed0')\n",
    "\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "print(medium.shape)\n",
    "inners=medium.transpose().conjugate()@medium\n",
    "print(inners.shape)\n",
    "coherence=0\n",
    "for i in inners:\n",
    "    for j in i:\n",
    "        if abs(j)>coherence and j<.99:\n",
    "            coherence=abs(j)\n",
    "print('coherence of data: ', coherence)\n",
    "\n",
    "\n",
    "Track_run=False\n",
    "#Enter your wanbd key below and uncomment wanbd code to track run on wandb\n",
    "if Track_run:\n",
    "    key='89a70fbc572a495206df640bd6c9cbf2a4a0dcaa' #enter your own key here\n",
    "    wandb.login(key=key) \n",
    "    wandb.init(project='rtt unlabeled', name=f'Classical Gelma, longer')\n",
    "\n",
    "    wandb.config['L1 final weight']=L1_weight_final\n",
    "    wandb.config['unlabeled_data']=unlabeled_data \n",
    "    wandb.config['L1_weight']=L1_weight\n",
    "    wandb.config['layers']=layers\n",
    "    wandb.config['GELMA']=GELMA\n",
    "    wandb.config['G_0']=G_0\n",
    "    wandb.config['GELMA_layers']=GELMA_layers   \n",
    "    wandb.config['inv_weight']=INV_weight\n",
    "    wandb.config['CE']=CE\n",
    "wand_dict={}\n",
    "#GELMA_layers.append(500)\n",
    "#layers.append(500)\n",
    "training_data=H.data_rho_loaded(data_path+'/train',unlabeled_data/80000,pixels=pixels, normalize=NORMALIZE)\n",
    "medium= np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt'])\n",
    "sigmoid = nn.Sigmoid()\n",
    "softmax=nn.Softmax(dim=0)\n",
    "#training_data.b=training_data.b.to(device)\n",
    "#training_data.rho=training_data.rho.to(device)\n",
    "#rh_t, _=torch.split(rho, 400, dim=-1)\n",
    "\n",
    "\n",
    "val_data=H.data_rho_loaded(data_path+'/val', 3000/80000, pixels=pixels, normalize=NORMALIZE)\n",
    "b_val=val_data.b.to(device)\n",
    "rho_val=val_data.rho.to(device)\n",
    "indim=int(training_data.b[0].shape[0]/2)\n",
    "outdim=(training_data.rho[0].shape[0]/4)\n",
    "enc_dim=training_data.b[0].shape[0]/2\n",
    "if KM_in:\n",
    "    enc_dim=enc_dim+outdim*2\n",
    "if pixels=='Gaussian_abs':\n",
    "    enc_out_dim=outdim*2\n",
    "    dec_in=int(training_data.rho[0].shape[0])\n",
    "\n",
    "else:\n",
    "    enc_out_dim=outdim\n",
    "    dec_in=int(training_data.rho[0].shape[0]/2)\n",
    "decoder_hats=[]\n",
    "encoder=M.fc_net_extra(enc_dim, layers,enc_out_dim, net_type='fc',linear_type='real', activation='leaky', bias=True, out_scaling=None,dropout=.5)\n",
    "#encoder=M.channeled_lin_layers_avg(enc_dim, layers, 10,int(outdim*2), dropout=.5)\n",
    "\n",
    "print(H.count_parameters(encoder))\n",
    "#decoder=nn.Linear(int(training_data.rho[0].shape[0]/2), int(training_data.b[0].shape[0]), bias=False)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "#decoder=M.norm_linear(int(training_data.rho[0].shape[0]), int(training_data.b[0].shape[0]), normalize=False)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "decoder=M.norm_linear_complex(int(training_data.rho[0].shape[0]/2), int(training_data.b[0].shape[0]), normalize=NORMALIZE)  #R^ {N_k} -> C^{N_rec*N_freq}\n",
    "\n",
    "#decoder=M.fc_net_extra(outdim, layers[::-1], indim, net_type='fc',linear_type='real', activation='relu', bias=True, out_scaling=None,dropout=.5)\n",
    "if G_0:\n",
    "    #G_0_w=(np.array(mat73.loadmat(data_path+'/G_0.mat')['A0']))\n",
    "    \n",
    "    G_0_w_np=(np.array(mat73.loadmat(data_path+'/rtt.mat')['Artt']))\n",
    "    G_0_w_np=G_0_w_np+np.random.normal(0, .01, G_0_w_np.shape)+1j*np.random.normal(0, .01, G_0_w_np.shape)\n",
    "    \n",
    "    #G_0_w_np=torch.tensor(G_0_w_np).float()\n",
    "\n",
    "\n",
    "    #G_0_w=torch.cat((torch.tensor(G_0_w_np.real), torch.tensor(G_0_w_np.imag)), dim=0)\n",
    "    #G_0_w_2=torch.cat((torch.tensor(G_0_w_np.real), -torch.tensor(G_0_w_np.imag)), dim=0)\n",
    "    #G_0_w=torch.cat((G_0_w, G_0_w_2), dim=1)\n",
    "    #G_0_w=G_0_w.float()\n",
    "    decoder.weight_real.data=nn.parameter.Parameter(torch.tensor(G_0_w_np.real).float().clone().detach().requires_grad_(True))\n",
    "    decoder.weight_imag.data=nn.parameter.Parameter(torch.tensor(G_0_w_np.imag).float().clone().detach().requires_grad_(True))\n",
    "\n",
    "    if pixels=='Gaussian_abs':\n",
    "        Complex_eye=torch.cat((torch.eye(int(outdim*2)), torch.zeros((int(outdim*2),int(outdim*2)))), dim=1)\n",
    "    else:\n",
    "        Complex_eye=torch.eye(int(outdim*2)).unsqueeze(1)\n",
    "    #Complex_eye=torch.eye(int(outdim*4))\n",
    "    Complex_eye=torch.cat((torch.eye(int(outdim*2)), torch.zeros((int(outdim*2),int(outdim*2)))), dim=1)\n",
    "    \n",
    "#    print(Complex_eye.shape,    G_0_w.shape)\n",
    "    medium_hat=decoder(Complex_eye).squeeze()\n",
    "    medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "\n",
    "\n",
    "\n",
    "print(H.count_parameters(encoder))\n",
    "if Track_run:\n",
    "    wandb.config['encoder params']=H.count_parameters(encoder)\n",
    "print('Data shapes:', training_data.b[0].shape[0]/2, training_data.rho[0].shape[0]/4)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "relu=nn.ReLU()\n",
    "leaky_relu=nn.LeakyReLU(THRESH_HOLD_VAL)\n",
    "optimizer = torch.optim.AdamW(encoder.parameters(), lr=LR,betas=betas, amsgrad=AMSGRAD,  weight_decay=1e-5)\n",
    "optimizer_decod = torch.optim.AdamW(decoder.parameters(), lr=LR, betas=betas, amsgrad=AMSGRAD, weight_decay=1e-5)\n",
    "True_l1_avg=0\n",
    "\n",
    "tanh=nn.Tanh()\n",
    "softmax=nn.Softmax(dim=1)\n",
    "bce_loss_fn=nn.BCELoss() \n",
    "Threshold=nn.Threshold(THRESH_HOLD_VAL, 0)  \n",
    "L2_loss_fn=nn.MSELoss()\n",
    "CE_loss_fn=nn.CrossEntropyLoss()\n",
    "#L2_loss_fn=lambda x,y: torch.sqrt(nn.MSELoss()(x,y))\n",
    "L1_loss_fn=nn.L1Loss()\n",
    "if pixels=='Gaussian_abs':\n",
    "    Complex_eye=torch.cat((torch.eye(int(outdim*2)), torch.zeros((int(outdim*2),int(outdim*2)))), dim=1).to(device)\n",
    "else:\n",
    "    Complex_eye=torch.eye(int(outdim*2)).unsqueeze(1).to(device)\n",
    "softmax=nn.Softmax(dim=1)\n",
    "Complex_eye=torch.eye(int(outdim*4)).unsqueeze(1).to(device)\n",
    "\n",
    "if GELMA>0:\n",
    "    GELMA_net=M.fc_net_extra(training_data.b[0].shape[0]/2, GELMA_layers, training_data.b[0].shape[0]/2, net_type='fc',linear_type='real', activation='leaky', bias=True, out_scaling=None, dropout=.5)\n",
    "    optimizer_GELMA = torch.optim.AdamW(GELMA_net.parameters(), lr=LR, maximize=True, betas=betas, amsgrad=AMSGRAD, weight_decay=1e-5)\n",
    "    lr_scheduler_GELMA=torch.optim.lr_scheduler.ExponentialLR(optimizer_GELMA, gamma=(LR_final/LR)**(1/Epochs), last_epoch=-1)\n",
    "    GELMA_net.to(device)\n",
    "    #GELMA_net=nn.DataParallel(GELMA_net)\n",
    "    GELMA_net.train()\n",
    "    #GELMA_net.set_params_to_zero()\n",
    "    #encoder.set_params_to_zero()\n",
    "def f_col(batch):\n",
    "    b=torch.stack([item[0] for item in batch])\n",
    "    rho=torch.stack([item[1] for item in batch])\n",
    "    num_targets=torch.stack([item[2] for item in batch])\n",
    "    return b.to(device), rho.to(device), num_targets\n",
    "\n",
    "\n",
    "def rescale_rho(z):\n",
    "    minimum, _=torch.min(z, -1, keepdim=True)\n",
    "    maxium, _=torch.max(z, -1,   keepdim=True)\n",
    "    z=(z-minimum)/(maxium-minimum)\n",
    "    return z\n",
    "lr_scheduler_enc=torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=(LR_final/LR)**(1/Epochs), last_epoch=-1)\n",
    "lr_scheduler_decod=torch.optim.lr_scheduler.ExponentialLR(optimizer_decod, gamma=(LR_final/LR)**(1/Epochs), last_epoch=-1)\n",
    "trainloader=DataLoader(training_data,batch_size=batchsize,shuffle=True,num_workers=0)\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    if RESETS>0 and epoch%RESETS==0:\n",
    "        encoder=M.fc_net_extra(enc_dim, layers,outdim, net_type='fc',linear_type='real', activation='leaky', bias=True, out_scaling=None,dropout=.5)\n",
    "        encoder.to(device)\n",
    "        optimizer = torch.optim.AdamW(encoder.parameters(), lr=LR)\n",
    "        optimizer_decod = torch.optim.AdamW(decoder.parameters(), lr=LR)\n",
    "    train_lossavg=0\n",
    "    train_lossavg=0\n",
    "    L1_loss_avg=0\n",
    "    coupled_grad=0\n",
    "    project_grad=0\n",
    "    INV_loss_avg=0\n",
    "    CEloss_avg=0\n",
    "\n",
    "\n",
    "    inner_loss_term_avg=0\n",
    "    True_l1_avg=0\n",
    "    for batch, (b, rho, num_targets) in enumerate(trainloader):\n",
    "        if True:\n",
    "            b=b.to(device)\n",
    "            rho=rho.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            optimizer_decod.zero_grad()\n",
    "            rho_hat=encoder(b)\n",
    "            rho_hat=soft_threh(rho_hat, THRESH_HOLD_VAL)\n",
    "            #rho_hat=relu(abs(rho_hat)-.005)*torch.sign(rho_hat)            \n",
    "            #rho_hat=softmax(rho_hat)\n",
    "            #max_rho, _=torch.max(abs(rho_hat), dim=-1, keepdim=True)\n",
    "            #rho_hat=abs(rho_hat)/max_rho\n",
    "            #rho_hat=sigmoid(rho_hat)\n",
    "            b_hat=decoder(rho_hat)\n",
    "            if L1_weight>0:\n",
    "                #L1_rhoreal, L1_rhoimag=torch.split(rho_hat, int(rho_hat.shape[-1]/2), dim=-1)\n",
    "                #L1_rho=(L1_rhoreal**2+L1_rhoimag**2)**.5\n",
    "                True_l1=L1_loss_fn(rho,rho*0)\n",
    "                True_l1_avg+=True_l1.item()/len(trainloader)\n",
    "                \n",
    "                L1_loss=L1_weight*L1_loss_fn(rho_hat, rho_hat*0)\n",
    "                L1_loss.backward(retain_graph=True)  \n",
    "                L1_loss_avg+=L1_loss.item()/L1_weight/len(trainloader)\n",
    "            else:\n",
    "                L1_loss=-1\n",
    "            L2_loss=L2_loss_fn(b_hat.squeeze(), b.squeeze())\n",
    "#            L2_loss=L2_loss_fn(F.normalize(b_hat.squeeze(), dim=-1), F.normalize(b.squeeze()))\n",
    "            L2_loss.backward(retain_graph=True)\n",
    "            train_lossavg+=L2_loss.item()/len(trainloader)\n",
    "\n",
    "            #if GELMA==0:\n",
    "            \n",
    "            if GELMA==0:\n",
    "                optimizer.step()\n",
    "                optimizer_decod.step()\n",
    "                inner_loss_term_avg=-1\n",
    "\n",
    "\n",
    "            elif GELMA>0 and epoch>0:\n",
    "                #optimizer_GELMA.zero_grad()\n",
    "                #optimizer.zero_grad()\n",
    "                #optimizer_decod.zero_grad()\n",
    "                \n",
    "\n",
    "\n",
    "                \n",
    "                GELMA_out=GELMA_net(b).squeeze()\n",
    "                GELMA_out=soft_threh(GELMA_out, THRESH_HOLD_VAL)\n",
    "                #GELMA_out=F.normalize(GELMA_out, dim=-1).squeeze()\n",
    "                b_hat=b_hat.squeeze()\n",
    "                \n",
    "                GELMA_inners_coupled=(torch.inner((-b_hat).squeeze(),GELMA_out.squeeze()).diagonal(dim1=-2, dim2=-1))\n",
    "                GELMA_inners_coupled=sum(GELMA_inners_coupled)/(torch.numel(GELMA_inners_coupled))/b.shape[-1]\n",
    "                GELMA_inners_project=(torch.inner((b).squeeze(),GELMA_out.squeeze()).diagonal(dim1=-2, dim2=-1))\n",
    "                GELMA_inners_project=sum(GELMA_inners_project)/(torch.numel(GELMA_inners_project))/b.shape[-1]\n",
    "                GELMA_inners=GELMA*(GELMA_inners_project+GELMA_inners_coupled)\n",
    "            \n",
    "\n",
    "                GELMA_inners.backward(retain_graph=True)          \n",
    "                inner_loss_term_avg+=GELMA_inners.item()/len(trainloader)/GELMA\n",
    "                optimizer_decod.step()\n",
    "            \n",
    "\n",
    "\n",
    "                optimizer_GELMA.step()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                optimizer_GELMA.zero_grad()\n",
    "                optimizer.zero_grad()\n",
    "                optimizer_decod.zero_grad()\n",
    "\n",
    "          \n",
    "\n",
    "    if L1_weight>0:\n",
    "        L1_weight=min(L1_weight*(L1_weight_final/L1_weight_og)**(1/(Epochs/2)), L1_weight_final)\n",
    "    if GELMA>0:\n",
    "        GELMA=min(GELMA*((GELMA_Final/GELMA_OG)**(1/(Epochs/2))), GELMA_Final)\n",
    "    if Scheduling and epoch>Epochs/2:\n",
    "        lr_scheduler_enc.step()\n",
    "        lr_scheduler_decod.step()\n",
    "        if GELMA>0:\n",
    "            lr_scheduler_GELMA.step()\n",
    "    val_lossavg=0\n",
    "    if epoch%50==49:\n",
    "        if pixels=='Gaussian_abs':\n",
    "            rho_hat=torch.abs(H.cat2complex(rho_hat.squeeze()))\n",
    "        H.plot_2_imgs(rho, rho_hat,ind=5, figsize=5, scaling=None, xpix=19, ypix=19, font_size=25)\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    #if GELMA_inc>0 and GELMA<GELMA_Final:\n",
    "    #    GELMA=GELMA*GELMA_inc\n",
    "    #    if GELMA>GELMA_Final:\n",
    "    #        GELMA=GELMA_Final\n",
    "    #        print('GELMA max reached')\n",
    "\n",
    "    if INV_weight>0 and INV_weight<1e-1:\n",
    "        INV_weight=INV_weight*(1.03)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    optimizer_decod.zero_grad()\n",
    "\n",
    "    font_size = 50\n",
    "    if KM_in:\n",
    "        medium_hat=decoder(Complex_eye).squeeze()\n",
    "        medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "        Mhat_real, Mhat_imag=torch.split(medium_hat, int(medium_hat.shape[1]/2), dim=-1)\n",
    "        Mhat_imag=-Mhat_imag\n",
    "        b_real, b_imag=torch.split(b_val.squeeze(), int(b_val.shape[-1]/2), dim=-1)\n",
    "        km_b_real=Mhat_real.squeeze()@b_real.squeeze().T-Mhat_imag.squeeze()@b_imag.squeeze().T\n",
    "        km_b_imag=Mhat_real.squeeze()@b_imag.squeeze().T+Mhat_imag.squeeze()@b_real.squeeze().T\n",
    "\n",
    "        km_b=torch.cat((km_b_real.T, km_b_imag.T), dim=-1)\n",
    "        \n",
    "        In_put=torch.cat((b_val, km_b), dim=-1)\n",
    "\n",
    "        rho_hat=encoder(In_put)\n",
    "    else:\n",
    "        rho_hat=encoder(b_val)\n",
    "        rho_hat=soft_threh(rho_hat, THRESH_HOLD_VAL)\n",
    "\n",
    "        #rhos_hat=relu(abs(rho_hat)-.005)*torch.sign(rho_hat)            \n",
    "\n",
    "    #rho_hat=sigmoid(rho_hat)\n",
    "    #rho_hat=relu(rho_hat)\n",
    "    #rho_hat=leaky_relu(rho_hat)\n",
    "    #rho_hat=softmax(rho_hat)\n",
    "\n",
    "    #max_rho, _=torch.max(rho_hat, dim=-1, keepdim=True)\n",
    "    #rho_hat=rho_hat/max_rho\n",
    "    #rho_hat=rescale_rho(rho_hat)\n",
    "    #rho_hat=Threshold(abs(rho_hat))\n",
    "    #max_rho, _=torch.max(rho_hat, dim=-1, keepdim=True) f\n",
    "    #rho_hat=rho_hat/max_rho\n",
    "    #rho_hat=tanh(rho_hat)\n",
    "\n",
    "    b_hat=decoder(rho_hat) \n",
    "  #  L2_loss=L2_loss_fn(F.normalize(b_hat.squeeze(), dim=-1), F.normalize(b_val.squeeze()))\n",
    "\n",
    "    L2_loss=L2_loss_fn(b_hat.squeeze(), b_val.squeeze())\n",
    "    val_lossavg+=L2_loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if epoch%50==49:\n",
    "        if pixels=='Gaussian_abs':\n",
    "            rho_hat=torch.abs(H.cat2complex(rho_hat.squeeze()))\n",
    "        H.plot_2_imgs(rho_val, rho_hat,ind=5, figsize=5, scaling=None, xpix=19, ypix=19, font_size=25)\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        sum_max_inner_original=0\n",
    "        medium_hat=decoder(Complex_eye).squeeze()\n",
    "        medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "        medium_hat=medium_hat.cpu().detach().numpy()\n",
    "        medium_hat=H.cat2complex(medium_hat)                    \n",
    "        torch_inners=np.abs(np.inner(medium_hat,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "                #original way of computing inners\n",
    "                # mean over true max over hats\n",
    "        index_list=[]\n",
    "        for i in range(int(outdim*2)):\n",
    "            if max(torch_inners[:, i])>.99 and i not in index_list:\n",
    "                index_list.append(i)\n",
    "            sum_max_inner_original=sum_max_inner_original+max(torch_inners[:, i])\n",
    "        max_avg_inners_original=sum_max_inner_original/(int(outdim*2))\n",
    "\n",
    "        \n",
    "        #lr_scheduler_enc.step()\n",
    "        #lr_scheduler_decod.step()\n",
    "        \n",
    "        \n",
    "        print(f'epoch: {epoch}, train loss: {train_lossavg}, L1 loss: {L1_loss_avg}, val loss: {val_lossavg}, max avg inners: {max_avg_inners_original}, GELMA loss: {inner_loss_term_avg}, num indices: {len(index_list)}, true L1 loss: {True_l1_avg}')\n",
    "        #print('True L1 loss:', True_l1_avg)\n",
    "        #print(f'coupled grad: {coupled_grad}, project grad: {project_grad}')\n",
    "        wand_dict['train loss']=train_lossavg\n",
    "        wand_dict['val loss']=val_lossavg\n",
    "        if L1_loss>0:\n",
    "            wand_dict['L1 loss']=L1_loss_avg\n",
    "        if GELMA>0:\n",
    "            wand_dict['GELMA loss']=inner_loss_term_avg\n",
    "        if CE>0:\n",
    "            wand_dict['CE loss']=CEloss_avg\n",
    "        if INV_weight>0:\n",
    "            wand_dict['INV loss']=INV_loss_avg\n",
    "\n",
    "    \n",
    "        wand_dict['max avg inners']=max_avg_inners_original\n",
    "        wand_dict['num indices']=len(index_list)\n",
    "    if Track_run:\n",
    "        wandb.log(wand_dict)\n",
    "\n",
    "print(time.time()-starttime)\n",
    "if Track_run:\n",
    "    wandb.finish()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#rho_hat.ravel().cpu().detach().numpy()\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrho_hat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "#rho_hat.ravel().cpu().detach().numpy()\n",
    "rho_hat.nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0.shape[0]/2, rho0.shape[0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium.shape, medium_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_max_inner_original=0\n",
    "medium_hat=decoder(Complex_eye.squeeze()).squeeze()\n",
    "medium_hat=F.normalize(medium_hat, dim=-1)\n",
    "medium_hat=medium_hat.cpu().detach().numpy()\n",
    "medium_hat=H.cat2complex(medium_hat)                    \n",
    "torch_inners=np.abs(np.inner(medium_hat,medium.T.conj()) ) #entry i j is ghat_i dot bar(g_j) \n",
    "        \n",
    "index_list=[]\n",
    "for i in range(int(outdim*2)):\n",
    "    if (np.max(torch_inners[:, i]))>.99 and i not in index_list:\n",
    "        index_list.append(i)\n",
    "    sum_max_inner_original=sum_max_inner_original+np.max(torch_inners[:, i])\n",
    "max_avg_inners_original=sum_max_inner_original/(int(outdim*2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workstation1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
